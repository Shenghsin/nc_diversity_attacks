{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Reloading any code written in external .py files.\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import datetime\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!  Training on GPU ...\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 5\n",
    "batch_size_train = 64\n",
    "batch_size_test = 100\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "log_interval = 100\n",
    "\n",
    "random_seed = 1\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "# check if CUDA is available\n",
    "device = torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    print('CUDA is available!  Training on GPU ...')\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print('CUDA is not available.  Training on CPU ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.MNIST('/data/', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "    batch_size=batch_size_train, shuffle=True, pin_memory=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.MNIST('/data/', train=False, download=True,\n",
    "                         transform=torchvision.transforms.Compose([\n",
    "                           torchvision.transforms.ToTensor(),\n",
    "                           torchvision.transforms.Normalize(\n",
    "                             (0.1307,), (0.3081,))\n",
    "                         ])),\n",
    "    batch_size=batch_size_test, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DenseNet, self).__init__()\n",
    "        self.dens1 = nn.Linear(784, 256)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.drop1 = nn.Dropout(0.2)\n",
    "        self.dens2 = nn.Linear(256, 128)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.drop2 = nn.Dropout(0.2)\n",
    "        self.dens3 = nn.Linear(128, 64)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.drop3 = nn.Dropout(0.2)\n",
    "        self.dens4 = nn.Linear(64, 20)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.drop4 = nn.Dropout(0.2)\n",
    "        self.dens5 = nn.Linear(20, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dens1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.drop1(x)\n",
    "        x = self.dens2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.drop2(x)\n",
    "        x = self.dens3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.drop3(x)\n",
    "        x = self.dens4(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.drop4(x)\n",
    "        x = self.dens5(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "    def extract_outputs(self, x, layer, neuron=None):\n",
    "        outputs = []\n",
    "        \n",
    "        def hook(module, input, output):\n",
    "            outputs.append(output)    \n",
    "            \n",
    "        for name, module in self.named_children():\n",
    "            if name == layer:\n",
    "                handle = module.register_forward_hook(hook)   \n",
    "                \n",
    "        out = self(x)\n",
    "        \n",
    "        if not neuron is None:\n",
    "            outputs[0] = outputs[0][0][neuron]\n",
    "   \n",
    "        handle.remove()\n",
    "  \n",
    "        return outputs\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(4 * 4 * 50, 500)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4 * 4 * 50)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "    def extract_outputs(self, x, layer, neuron=None):\n",
    "        outputs = []      \n",
    "        def hook(module, input, output):\n",
    "            outputs.append(output)    \n",
    "        for name, module in self.named_children():\n",
    "            if name == layer:\n",
    "                handle = module.register_forward_hook(hook)     \n",
    "        out = self(x)\n",
    "        if not neuron is None:\n",
    "            outputs[0] = outputs[0][0][neuron]\n",
    "        else:\n",
    "            outputs[0] = outputs[0][0]\n",
    "        handle.remove()\n",
    "        return outputs\n",
    "  \n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        # calculate robust loss\n",
    "        loss = F.cross_entropy(model(data), target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                       100. * batch_idx / len(train_loader), loss.item()))\n",
    "            \n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.300039\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.437823\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.366197\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.324132\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.223611\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.338487\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.063627\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.114942\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.144618\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.117394\n",
      "\n",
      "Test set: Average loss: 0.1022, Accuracy: 9657/10000 (97%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.148142\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.103152\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.059494\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.065197\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.119787\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.034771\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.034602\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.038330\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.028553\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.038750\n",
      "\n",
      "Test set: Average loss: 0.0611, Accuracy: 9828/10000 (98%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.052121\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.098551\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.033779\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.068480\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.033973\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.027600\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.022182\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.059063\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.048870\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.014959\n",
      "\n",
      "Test set: Average loss: 0.0564, Accuracy: 9814/10000 (98%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.020500\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.023816\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.076641\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.025822\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.016935\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.014757\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.010377\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.068100\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.013873\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.060521\n",
      "\n",
      "Test set: Average loss: 0.0408, Accuracy: 9863/10000 (99%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.010507\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.057262\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.064902\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.051254\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.024645\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.041470\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.057718\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.179015\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.073363\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.030117\n",
      "\n",
      "Test set: Average loss: 0.0386, Accuracy: 9871/10000 (99%)\n",
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'pretrained_models/mnist/model_ConvNet_2019-07-31 19.16.28.445851_None.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-45426110f7e1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m     \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'pretrained_models/mnist/model_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mm_type\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m':'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'.'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0macc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.pth'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\summer19_research\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol)\u001b[0m\n\u001b[0;32m    222\u001b[0m         \u001b[1;33m>>\u001b[0m\u001b[1;33m>\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \"\"\"\n\u001b[1;32m--> 224\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_with_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"wb\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0m_save\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\summer19_research\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_with_file_like\u001b[1;34m(f, mode, body)\u001b[0m\n\u001b[0;32m    145\u001b[0m             \u001b[1;33m(\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m3\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 147\u001b[1;33m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    148\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'pretrained_models/mnist/model_ConvNet_2019-07-31 19.16.28.445851_None.pth'"
     ]
    }
   ],
   "source": [
    "retrain = False\n",
    "track_low_high = False\n",
    "\n",
    "model = ConvNet().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "# check to see if we can just load a previous model\n",
    "# %mkdir models\n",
    "latest_model = None\n",
    "m_type = model.__class__.__name__\n",
    "prev_models = glob.glob('pretrained_models/mnist/*'+ m_type +'*.pth')\n",
    "if prev_models:\n",
    "    latest_model = max(prev_models, key=os.path.getctime)\n",
    "\n",
    "if (retrain is False \n",
    "    and latest_model is not None \n",
    "    and m_type in latest_model):  \n",
    "    print('loading model', latest_model)\n",
    "    model.load_state_dict(torch.load(latest_model))  \n",
    "else:\n",
    "    if track_low_high:\n",
    "        model.init_dict(model.lowhigh_dict, inputs, 'relu', {'low': 0, 'high': 0})\n",
    "        try:\n",
    "            for epoch in range(1, n_epochs + 1):\n",
    "                model.hook_lowhigh_dict('relu')\n",
    "                train(model, device, train_loader, optimizer, epoch)\n",
    "                model.remove_hooks()\n",
    "                test(model, device, test_loader)    \n",
    "        finally:\n",
    "            model.remove_hooks()   \n",
    "    else:\n",
    "        for epoch in range(1, n_epochs + 1):\n",
    "            train(model, device, train_loader, optimizer, epoch)\n",
    "            acc = test(model, device, test_loader)  \n",
    "    torch.save(model.state_dict(), 'pretrained_models/mnist/model_' + m_type + '_' + str(datetime.datetime.now()).replace(':','.') + '_' + str(acc) + '.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = test_loader # the dataloader (of type torch.utils.data.DataLoader)\n",
    "mean = (0.1307,) # the mean used in inputs normalization\n",
    "std = (0.3081,) # the standard deviation used in inputs normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, targets = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_samples = 4\n",
    "for i in range(1,num_samples+1):\n",
    "    plt.subplot(1, num_samples, i)\n",
    "    plt.imshow(np.squeeze(inputs[i].numpy()))\n",
    "    plt.title(targets[i].item())\n",
    "    plt.tight_layout()\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fabrice\\Dropbox\\UCLA\\research\\19 Summer\\diversity attacks\\cw attack\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "\n",
    "model = model.to(device)\n",
    "inputs = inputs.to(device)\n",
    "targets = targets.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 Using scale consts: [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]\n",
      "batch [0] loss: 0.8916170597076416\n",
      "batch [10] loss: 0.892185628414154\n",
      "batch [20] loss: 0.89176344871521\n",
      "batch [30] loss: 0.8915929794311523\n",
      "batch [40] loss: 0.8915765881538391\n",
      "batch [50] loss: 0.8915737271308899\n",
      "batch [60] loss: 0.8915719985961914\n",
      "batch [70] loss: 0.8915715217590332\n",
      "batch [80] loss: 0.8915712833404541\n",
      "batch [90] loss: 0.891571044921875\n",
      "batch [100] loss: 0.8915709257125854\n",
      "Step 1 Using scale consts: [0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.0005, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
      "batch [0] loss: 8.915295600891113\n",
      "batch [10] loss: 8.912496566772461\n",
      "batch [20] loss: 8.911946296691895\n",
      "batch [30] loss: 8.911738395690918\n",
      "batch [40] loss: 8.911650657653809\n",
      "batch [50] loss: 8.911611557006836\n",
      "batch [60] loss: 8.911592483520508\n",
      "batch [70] loss: 8.911582946777344\n",
      "batch [80] loss: 8.911577224731445\n",
      "batch [90] loss: 8.91157341003418\n",
      "batch [100] loss: 8.911571502685547\n",
      "batch [110] loss: 8.911569595336914\n",
      "batch [120] loss: 8.911567687988281\n",
      "batch [130] loss: 8.911566734313965\n",
      "batch [140] loss: 8.911566734313965\n",
      "batch [150] loss: 8.911567687988281\n",
      "batch [160] loss: 8.911565780639648\n",
      "batch [170] loss: 8.911565780639648\n",
      "batch [180] loss: 8.911565780639648\n",
      "batch [190] loss: 8.911564826965332\n",
      "batch [200] loss: 8.911565780639648\n",
      "Step 2 Using scale consts: [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.00025, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]\n",
      "batch [0] loss: 89.07427215576172\n",
      "batch [10] loss: 88.88037872314453\n",
      "batch [20] loss: 88.83041381835938\n",
      "batch [30] loss: 88.8021240234375\n",
      "batch [40] loss: 88.78466033935547\n",
      "batch [50] loss: 88.77266693115234\n",
      "batch [60] loss: 88.76375579833984\n",
      "batch [70] loss: 88.75675964355469\n",
      "batch [80] loss: 88.75105285644531\n",
      "batch [90] loss: 88.74629974365234\n",
      "batch [100] loss: 88.74223327636719\n",
      "batch [110] loss: 88.73873901367188\n",
      "batch [120] loss: 88.73567962646484\n",
      "batch [130] loss: 88.73298645019531\n",
      "batch [140] loss: 88.73059844970703\n",
      "batch [150] loss: 88.7284927368164\n",
      "batch [160] loss: 88.7265853881836\n",
      "batch [170] loss: 88.72486114501953\n",
      "batch [180] loss: 88.72332000732422\n",
      "batch [190] loss: 88.721923828125\n",
      "batch [200] loss: 88.72064208984375\n",
      "batch [210] loss: 88.719482421875\n",
      "batch [220] loss: 88.71839904785156\n",
      "batch [230] loss: 88.71741485595703\n",
      "batch [240] loss: 88.71652221679688\n",
      "batch [250] loss: 88.71569061279297\n",
      "batch [260] loss: 88.71492004394531\n",
      "batch [270] loss: 88.71421813964844\n",
      "batch [280] loss: 88.71355438232422\n",
      "batch [290] loss: 88.71294403076172\n",
      "batch [300] loss: 88.71238708496094\n",
      "Step 3 Using scale consts: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.000125, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "batch [0] loss: 883.415771484375\n",
      "batch [10] loss: 874.60986328125\n",
      "batch [20] loss: 869.4568481445312\n",
      "batch [30] loss: 866.7343139648438\n",
      "batch [40] loss: 865.0474853515625\n",
      "batch [50] loss: 863.8650512695312\n",
      "batch [60] loss: 862.9688720703125\n",
      "batch [70] loss: 862.2456665039062\n",
      "batch [80] loss: 861.632568359375\n",
      "batch [90] loss: 861.0975952148438\n",
      "batch [100] loss: 860.6182861328125\n",
      "batch [110] loss: 860.1829833984375\n",
      "batch [120] loss: 859.7821044921875\n",
      "batch [130] loss: 859.4093017578125\n",
      "batch [140] loss: 859.0599365234375\n",
      "batch [150] loss: 858.7305908203125\n",
      "batch [160] loss: 858.4185791015625\n",
      "batch [170] loss: 858.1220092773438\n",
      "batch [180] loss: 857.8372192382812\n",
      "batch [190] loss: 857.5643310546875\n",
      "batch [200] loss: 857.302978515625\n",
      "batch [210] loss: 857.0518798828125\n",
      "batch [220] loss: 856.8095703125\n",
      "batch [230] loss: 856.57568359375\n",
      "batch [240] loss: 856.3496704101562\n",
      "batch [250] loss: 856.1309204101562\n",
      "batch [260] loss: 855.9180297851562\n",
      "batch [270] loss: 855.71142578125\n",
      "batch [280] loss: 855.51025390625\n",
      "batch [290] loss: 855.3142700195312\n",
      "batch [300] loss: 855.1224975585938\n",
      "batch [310] loss: 854.9363403320312\n",
      "batch [320] loss: 854.7550048828125\n",
      "batch [330] loss: 854.5780639648438\n",
      "batch [340] loss: 854.4046020507812\n",
      "batch [350] loss: 854.2345581054688\n",
      "batch [360] loss: 854.0693359375\n",
      "batch [370] loss: 853.9083251953125\n",
      "batch [380] loss: 853.751220703125\n",
      "batch [390] loss: 853.5971069335938\n",
      "batch [400] loss: 853.4473876953125\n",
      "batch [410] loss: 853.3016357421875\n",
      "batch [420] loss: 853.1585083007812\n",
      "batch [430] loss: 853.0189819335938\n",
      "batch [440] loss: 852.8826293945312\n",
      "batch [450] loss: 852.7496337890625\n",
      "batch [460] loss: 852.619873046875\n",
      "batch [470] loss: 852.493408203125\n",
      "batch [480] loss: 852.3698120117188\n",
      "batch [490] loss: 852.2483520507812\n",
      "batch [500] loss: 852.1307373046875\n",
      "batch [510] loss: 852.0155639648438\n",
      "batch [520] loss: 851.903076171875\n",
      "batch [530] loss: 851.7936401367188\n",
      "batch [540] loss: 851.6870727539062\n",
      "batch [550] loss: 851.5826416015625\n",
      "batch [560] loss: 851.480712890625\n",
      "batch [570] loss: 851.3817138671875\n",
      "batch [580] loss: 851.2853393554688\n",
      "batch [590] loss: 851.1915893554688\n",
      "batch [600] loss: 851.1004638671875\n",
      "batch [610] loss: 851.0118408203125\n",
      "batch [620] loss: 850.9252319335938\n",
      "batch [630] loss: 850.8407592773438\n",
      "batch [640] loss: 850.7581787109375\n",
      "batch [650] loss: 850.6776123046875\n",
      "batch [660] loss: 850.5995483398438\n",
      "batch [670] loss: 850.5238037109375\n",
      "batch [680] loss: 850.4497680664062\n",
      "batch [690] loss: 850.376708984375\n",
      "batch [700] loss: 850.3057861328125\n",
      "batch [710] loss: 850.2371215820312\n",
      "batch [720] loss: 850.1702880859375\n",
      "batch [730] loss: 850.1052856445312\n",
      "batch [740] loss: 850.0424194335938\n",
      "batch [750] loss: 849.9815673828125\n",
      "batch [760] loss: 849.9225463867188\n",
      "batch [770] loss: 849.8652954101562\n",
      "batch [780] loss: 849.808837890625\n",
      "batch [790] loss: 849.7543334960938\n",
      "batch [800] loss: 849.701171875\n",
      "batch [810] loss: 849.6495361328125\n",
      "batch [820] loss: 849.5997924804688\n",
      "batch [830] loss: 849.5516357421875\n",
      "batch [840] loss: 849.5052490234375\n",
      "batch [850] loss: 849.4590454101562\n",
      "batch [860] loss: 849.4148559570312\n",
      "batch [870] loss: 849.3720703125\n",
      "batch [880] loss: 849.3290405273438\n",
      "batch [890] loss: 849.2885131835938\n",
      "batch [900] loss: 849.2496337890625\n",
      "batch [910] loss: 849.2119750976562\n",
      "batch [920] loss: 849.17578125\n",
      "batch [930] loss: 849.1407470703125\n",
      "batch [940] loss: 849.10693359375\n",
      "batch [950] loss: 849.0744018554688\n",
      "batch [960] loss: 849.0429077148438\n",
      "batch [970] loss: 849.0126342773438\n",
      "batch [980] loss: 848.983154296875\n",
      "batch [990] loss: 848.954833984375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4 Using scale consts: [10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 6.25e-05, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0]\n",
      "batch [0] loss: 8142.20654296875\n",
      "batch [10] loss: 7973.3115234375\n",
      "batch [20] loss: 7790.81005859375\n",
      "batch [30] loss: 7641.19091796875\n",
      "batch [40] loss: 7518.7177734375\n",
      "batch [50] loss: 7416.2548828125\n",
      "batch [60] loss: 7327.33642578125\n",
      "batch [70] loss: 7248.7470703125\n",
      "batch [80] loss: 7179.0205078125\n",
      "batch [90] loss: 7115.89599609375\n",
      "batch [100] loss: 7057.80908203125\n",
      "batch [110] loss: 7004.20703125\n",
      "batch [120] loss: 6954.0693359375\n",
      "batch [130] loss: 6907.21435546875\n",
      "batch [140] loss: 6863.81298828125\n",
      "batch [150] loss: 6822.27587890625\n",
      "batch [160] loss: 6782.64013671875\n",
      "batch [170] loss: 6744.9638671875\n",
      "batch [180] loss: 6708.87890625\n",
      "batch [190] loss: 6673.9638671875\n",
      "batch [200] loss: 6640.03662109375\n",
      "batch [210] loss: 6607.0859375\n",
      "batch [220] loss: 6575.57470703125\n",
      "batch [230] loss: 6544.79931640625\n",
      "batch [240] loss: 6514.89111328125\n",
      "batch [250] loss: 6485.7021484375\n",
      "batch [260] loss: 6457.21484375\n",
      "batch [270] loss: 6429.66064453125\n",
      "batch [280] loss: 6402.79638671875\n",
      "batch [290] loss: 6376.65869140625\n",
      "batch [300] loss: 6351.22314453125\n",
      "batch [310] loss: 6326.35595703125\n",
      "batch [320] loss: 6302.02734375\n",
      "batch [330] loss: 6278.19873046875\n",
      "batch [340] loss: 6254.72509765625\n",
      "batch [350] loss: 6231.50341796875\n",
      "batch [360] loss: 6208.57470703125\n",
      "batch [370] loss: 6185.9189453125\n",
      "batch [380] loss: 6163.640625\n",
      "batch [390] loss: 6141.5537109375\n",
      "batch [400] loss: 6119.7958984375\n",
      "batch [410] loss: 6098.4580078125\n",
      "batch [420] loss: 6077.603515625\n",
      "batch [430] loss: 6057.2802734375\n",
      "batch [440] loss: 6037.3984375\n",
      "batch [450] loss: 6017.732421875\n",
      "batch [460] loss: 5998.57421875\n",
      "batch [470] loss: 5979.8427734375\n",
      "batch [480] loss: 5961.6787109375\n",
      "batch [490] loss: 5944.12939453125\n",
      "batch [500] loss: 5926.5908203125\n",
      "batch [510] loss: 5909.4169921875\n",
      "batch [520] loss: 5892.5791015625\n",
      "batch [530] loss: 5876.04638671875\n",
      "batch [540] loss: 5860.03125\n",
      "batch [550] loss: 5844.31689453125\n",
      "batch [560] loss: 5828.69873046875\n",
      "batch [570] loss: 5813.337890625\n",
      "batch [580] loss: 5798.185546875\n",
      "batch [590] loss: 5783.328125\n",
      "batch [600] loss: 5768.95849609375\n",
      "batch [610] loss: 5754.7783203125\n",
      "batch [620] loss: 5740.97900390625\n",
      "batch [630] loss: 5727.37255859375\n",
      "batch [640] loss: 5714.05810546875\n",
      "batch [650] loss: 5701.04541015625\n",
      "batch [660] loss: 5688.2509765625\n",
      "batch [670] loss: 5675.68310546875\n",
      "batch [680] loss: 5663.2705078125\n",
      "batch [690] loss: 5651.25439453125\n",
      "batch [700] loss: 5639.34326171875\n",
      "batch [710] loss: 5627.48095703125\n",
      "batch [720] loss: 5616.00634765625\n",
      "batch [730] loss: 5604.5927734375\n",
      "batch [740] loss: 5593.4248046875\n",
      "batch [750] loss: 5582.4892578125\n",
      "batch [760] loss: 5571.91796875\n",
      "batch [770] loss: 5561.48876953125\n",
      "batch [780] loss: 5551.2275390625\n",
      "batch [790] loss: 5541.16748046875\n",
      "batch [800] loss: 5531.23876953125\n",
      "batch [810] loss: 5521.62548828125\n",
      "batch [820] loss: 5512.2109375\n",
      "batch [830] loss: 5502.78857421875\n",
      "batch [840] loss: 5493.4765625\n",
      "batch [850] loss: 5484.2392578125\n",
      "batch [860] loss: 5475.130859375\n",
      "batch [870] loss: 5466.0859375\n",
      "batch [880] loss: 5457.28515625\n",
      "batch [890] loss: 5448.75048828125\n",
      "batch [900] loss: 5440.51318359375\n",
      "batch [910] loss: 5432.56982421875\n",
      "batch [920] loss: 5424.7421875\n",
      "batch [930] loss: 5417.0595703125\n",
      "batch [940] loss: 5409.5419921875\n",
      "batch [950] loss: 5402.13330078125\n",
      "batch [960] loss: 5394.86376953125\n",
      "batch [970] loss: 5387.7099609375\n",
      "batch [980] loss: 5380.69140625\n",
      "batch [990] loss: 5373.7236328125\n",
      "Step 5 Using scale consts: [100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 5.5, 100.0, 100.0, 100.0, 5.5, 100.0, 5.5, 100.0, 100.0, 5.5, 5.5, 100.0, 100.0, 5.5, 100.0, 100.0, 5.5, 5.5, 100.0, 100.0, 100.0, 100.0, 5.5, 5.5, 100.0, 100.0, 5.5, 5.5, 100.0, 5.5, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 3.125e-05, 5.5, 100.0, 5.5, 5.5, 100.0, 5.5, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 5.5, 100.0, 100.0, 100.0, 5.5, 100.0, 5.5, 5.5, 100.0, 5.5, 100.0, 5.5, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 5.5, 100.0, 5.5, 5.5, 100.0, 100.0, 100.0, 100.0, 5.5, 100.0, 5.5, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]\n",
      "batch [0] loss: 35102.62109375\n",
      "batch [10] loss: 33374.1875\n",
      "batch [20] loss: 31362.4140625\n",
      "batch [30] loss: 29525.63671875\n",
      "batch [40] loss: 27925.390625\n",
      "batch [50] loss: 26520.953125\n",
      "batch [60] loss: 25339.029296875\n",
      "batch [70] loss: 24324.873046875\n",
      "batch [80] loss: 23391.150390625\n",
      "batch [90] loss: 22575.58203125\n",
      "batch [100] loss: 21835.03515625\n",
      "batch [110] loss: 21152.6171875\n",
      "batch [120] loss: 20505.427734375\n",
      "batch [130] loss: 19867.478515625\n",
      "batch [140] loss: 19267.375\n",
      "batch [150] loss: 18706.328125\n",
      "batch [160] loss: 18175.849609375\n",
      "batch [170] loss: 17670.734375\n",
      "batch [180] loss: 17193.8828125\n",
      "batch [190] loss: 16741.091796875\n",
      "batch [200] loss: 16315.791015625\n",
      "batch [210] loss: 15917.8134765625\n",
      "batch [220] loss: 15532.580078125\n",
      "batch [230] loss: 15158.2099609375\n",
      "batch [240] loss: 14806.30859375\n",
      "batch [250] loss: 14487.240234375\n",
      "batch [260] loss: 14176.9150390625\n",
      "batch [270] loss: 13886.0830078125\n",
      "batch [280] loss: 13632.3740234375\n",
      "batch [290] loss: 13393.6884765625\n",
      "batch [300] loss: 13161.3798828125\n",
      "batch [310] loss: 12946.80078125\n",
      "batch [320] loss: 12770.474609375\n",
      "batch [330] loss: 12601.9150390625\n",
      "batch [340] loss: 12434.2177734375\n",
      "batch [350] loss: 12274.1748046875\n",
      "batch [360] loss: 12121.001953125\n",
      "batch [370] loss: 11969.4521484375\n",
      "batch [380] loss: 11821.568359375\n",
      "batch [390] loss: 11677.328125\n",
      "batch [400] loss: 11540.46484375\n",
      "batch [410] loss: 11426.919921875\n",
      "batch [420] loss: 11317.1279296875\n",
      "batch [430] loss: 11212.541015625\n",
      "batch [440] loss: 11110.640625\n",
      "batch [450] loss: 11012.0283203125\n",
      "batch [460] loss: 10914.279296875\n",
      "batch [470] loss: 10818.8671875\n",
      "batch [480] loss: 10729.123046875\n",
      "batch [490] loss: 10643.8232421875\n",
      "batch [500] loss: 10559.1572265625\n",
      "batch [510] loss: 10484.71875\n",
      "batch [520] loss: 10411.431640625\n",
      "batch [530] loss: 10338.48828125\n",
      "batch [540] loss: 10267.484375\n",
      "batch [550] loss: 10202.189453125\n",
      "batch [560] loss: 10140.2900390625\n",
      "batch [570] loss: 10083.94921875\n",
      "batch [580] loss: 10028.068359375\n",
      "batch [590] loss: 9973.1884765625\n",
      "batch [600] loss: 9922.8896484375\n",
      "batch [610] loss: 9876.9619140625\n",
      "batch [620] loss: 9832.0048828125\n",
      "batch [630] loss: 9787.8935546875\n",
      "batch [640] loss: 9750.54296875\n",
      "batch [650] loss: 9713.9150390625\n",
      "batch [660] loss: 9682.65625\n",
      "batch [670] loss: 9652.46875\n",
      "batch [680] loss: 9621.8427734375\n",
      "batch [690] loss: 9592.328125\n",
      "batch [700] loss: 9562.4267578125\n",
      "batch [710] loss: 9532.087890625\n",
      "batch [720] loss: 9501.7822265625\n",
      "batch [730] loss: 9473.0009765625\n",
      "batch [740] loss: 9447.603515625\n",
      "batch [750] loss: 9421.0888671875\n",
      "batch [760] loss: 9395.046875\n",
      "batch [770] loss: 9370.1767578125\n",
      "batch [780] loss: 9345.3037109375\n",
      "batch [790] loss: 9321.021484375\n",
      "batch [800] loss: 9297.359375\n",
      "batch [810] loss: 9273.046875\n",
      "batch [820] loss: 9250.0107421875\n",
      "batch [830] loss: 9227.1337890625\n",
      "batch [840] loss: 9204.583984375\n",
      "batch [850] loss: 9182.138671875\n",
      "batch [860] loss: 9159.61328125\n",
      "batch [870] loss: 9137.5712890625\n",
      "batch [880] loss: 9114.9501953125\n",
      "batch [890] loss: 9092.6982421875\n",
      "batch [900] loss: 9070.5546875\n",
      "batch [910] loss: 9048.751953125\n",
      "batch [920] loss: 9027.3837890625\n",
      "batch [930] loss: 9005.958984375\n",
      "batch [940] loss: 8985.1416015625\n",
      "batch [950] loss: 8964.2607421875\n",
      "batch [960] loss: 8944.546875\n",
      "batch [970] loss: 8924.1298828125\n",
      "batch [980] loss: 8906.1064453125\n",
      "batch [990] loss: 8888.9423828125\n",
      "Step 6 Using scale consts: [55.0, 55.0, 55.0, 1000.0, 55.0, 55.0, 55.0, 3.25, 55.0, 55.0, 55.0, 3.25, 1000.0, 3.25, 55.0, 1000.0, 7.75, 3.25, 55.0, 55.0, 3.25, 55.0, 55.0, 3.25, 3.25, 55.0, 55.0, 55.0, 55.0, 3.25, 3.25, 55.0, 55.0, 3.25, 3.25, 55.0, 3.25, 55.0, 55.0, 55.0, 55.0, 1000.0, 55.0, 55.0, 55.0, 55.0, 1.5625e-05, 3.25, 55.0, 3.25, 3.25, 55.0, 3.25, 55.0, 55.0, 55.0, 55.0, 55.0, 55.0, 55.0, 55.0, 3.25, 55.0, 55.0, 55.0, 3.25, 1000.0, 3.25, 3.25, 55.0, 3.25, 55.0, 3.25, 55.0, 55.0, 55.0, 55.0, 55.0, 1000.0, 55.0, 55.0, 3.25, 55.0, 3.25, 3.25, 55.0, 55.0, 55.0, 55.0, 3.25, 55.0, 3.25, 55.0, 55.0, 55.0, 55.0, 55.0, 55.0, 55.0, 1000.0]\n",
      "batch [0] loss: 33238.3203125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch [10] loss: 32363.38671875\n",
      "batch [20] loss: 31429.25\n",
      "batch [30] loss: 30735.771484375\n",
      "batch [40] loss: 30130.212890625\n",
      "batch [50] loss: 29798.634765625\n",
      "batch [60] loss: 29525.57421875\n",
      "batch [70] loss: 29282.279296875\n",
      "batch [80] loss: 29062.08984375\n",
      "batch [90] loss: 28858.4921875\n",
      "batch [100] loss: 28732.6953125\n",
      "batch [110] loss: 28618.314453125\n",
      "batch [120] loss: 28508.666015625\n",
      "batch [130] loss: 28403.3984375\n",
      "batch [140] loss: 28302.056640625\n",
      "batch [150] loss: 28203.86328125\n",
      "batch [160] loss: 28109.88671875\n",
      "batch [170] loss: 28020.796875\n",
      "batch [180] loss: 27935.73828125\n",
      "batch [190] loss: 27853.4375\n",
      "batch [200] loss: 27773.630859375\n",
      "batch [210] loss: 27697.33203125\n",
      "batch [220] loss: 27622.384765625\n",
      "batch [230] loss: 27552.365234375\n",
      "batch [240] loss: 27485.33203125\n",
      "batch [250] loss: 27420.6640625\n",
      "batch [260] loss: 27358.798828125\n",
      "batch [270] loss: 27297.791015625\n",
      "batch [280] loss: 27238.79296875\n",
      "batch [290] loss: 27181.396484375\n",
      "batch [300] loss: 27125.21484375\n",
      "batch [310] loss: 27070.544921875\n",
      "batch [320] loss: 27016.38671875\n",
      "batch [330] loss: 26964.06640625\n",
      "batch [340] loss: 26911.890625\n",
      "batch [350] loss: 26861.05078125\n",
      "batch [360] loss: 26811.306640625\n",
      "batch [370] loss: 26761.568359375\n",
      "batch [380] loss: 26713.34375\n",
      "batch [390] loss: 26664.115234375\n",
      "batch [400] loss: 26617.216796875\n",
      "batch [410] loss: 26569.4375\n",
      "batch [420] loss: 26522.51171875\n",
      "batch [430] loss: 26476.0390625\n",
      "batch [440] loss: 26430.4609375\n",
      "batch [450] loss: 26384.447265625\n",
      "batch [460] loss: 26338.65625\n",
      "batch [470] loss: 26294.818359375\n",
      "batch [480] loss: 26249.958984375\n",
      "batch [490] loss: 26205.97265625\n",
      "batch [500] loss: 26161.6328125\n",
      "batch [510] loss: 26116.9453125\n",
      "batch [520] loss: 26073.568359375\n",
      "batch [530] loss: 26029.046875\n",
      "batch [540] loss: 25985.630859375\n",
      "batch [550] loss: 25941.708984375\n",
      "batch [560] loss: 25897.509765625\n",
      "batch [570] loss: 25853.626953125\n",
      "batch [580] loss: 25809.884765625\n",
      "batch [590] loss: 25766.732421875\n",
      "batch [600] loss: 25722.853515625\n",
      "batch [610] loss: 25679.3984375\n",
      "batch [620] loss: 25636.08984375\n",
      "batch [630] loss: 25592.59765625\n",
      "batch [640] loss: 25549.923828125\n",
      "batch [650] loss: 25506.267578125\n",
      "batch [660] loss: 25462.96875\n",
      "batch [670] loss: 25419.87109375\n",
      "batch [680] loss: 25375.96875\n",
      "batch [690] loss: 25332.40625\n",
      "batch [700] loss: 25287.8671875\n",
      "batch [710] loss: 25243.82421875\n",
      "batch [720] loss: 25199.919921875\n",
      "batch [730] loss: 25155.23828125\n",
      "batch [740] loss: 25110.9921875\n",
      "batch [750] loss: 25065.93359375\n",
      "batch [760] loss: 25021.38671875\n",
      "batch [770] loss: 24976.53125\n",
      "batch [780] loss: 24931.91015625\n",
      "batch [790] loss: 24886.93359375\n",
      "batch [800] loss: 24841.28125\n",
      "batch [810] loss: 24795.083984375\n",
      "batch [820] loss: 24747.642578125\n",
      "batch [830] loss: 24700.9296875\n",
      "batch [840] loss: 24652.9765625\n",
      "batch [850] loss: 24604.962890625\n",
      "batch [860] loss: 24556.220703125\n",
      "batch [870] loss: 24506.515625\n",
      "batch [880] loss: 24456.955078125\n",
      "batch [890] loss: 24405.84375\n",
      "batch [900] loss: 24355.43359375\n",
      "batch [910] loss: 24303.267578125\n",
      "batch [920] loss: 24250.658203125\n",
      "batch [930] loss: 24197.677734375\n",
      "batch [940] loss: 24144.193359375\n",
      "batch [950] loss: 24089.044921875\n",
      "batch [960] loss: 24032.70703125\n",
      "batch [970] loss: 23976.724609375\n",
      "batch [980] loss: 23918.09765625\n",
      "batch [990] loss: 23858.330078125\n",
      "Step 7 Using scale consts: [32.5, 32.5, 32.5, 550.0, 32.5, 32.5, 32.5, 4.375, 32.5, 32.5, 32.5, 4.375, 550.0, 2.125, 32.5, 10000.0, 6.625, 4.375, 32.5, 32.5, 4.375, 32.5, 32.5, 4.375, 4.375, 32.5, 32.5, 32.5, 32.5, 2.125, 4.375, 32.5, 32.5, 4.375, 4.375, 32.5, 2.125, 32.5, 32.5, 32.5, 32.5, 10000.0, 32.5, 32.5, 32.5, 32.5, 7.8125e-06, 4.375, 32.5, 4.375, 2.125, 32.5, 4.375, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 2.125, 32.5, 32.5, 32.5, 4.375, 10000.0, 4.375, 2.125, 32.5, 4.375, 32.5, 2.125, 32.5, 32.5, 32.5, 32.5, 32.5, 10000.0, 32.5, 32.5, 4.375, 32.5, 2.125, 4.375, 32.5, 32.5, 32.5, 32.5, 4.375, 32.5, 4.375, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 32.5, 550.0]\n",
      "batch [0] loss: 181951.625\n",
      "batch [10] loss: 180311.296875\n",
      "batch [20] loss: 178224.15625\n",
      "batch [30] loss: 176197.421875\n",
      "batch [40] loss: 174308.578125\n",
      "batch [50] loss: 172548.15625\n",
      "batch [60] loss: 170859.015625\n",
      "batch [70] loss: 169226.15625\n",
      "batch [80] loss: 167627.34375\n",
      "batch [90] loss: 166054.09375\n",
      "batch [100] loss: 164516.390625\n",
      "batch [110] loss: 162996.75\n",
      "batch [120] loss: 161491.328125\n",
      "batch [130] loss: 159983.34375\n",
      "batch [140] loss: 158487.765625\n",
      "batch [150] loss: 157006.4375\n",
      "batch [160] loss: 155552.671875\n",
      "batch [170] loss: 154443.140625\n",
      "batch [180] loss: 153330.453125\n",
      "batch [190] loss: 152219.171875\n",
      "batch [200] loss: 151103.53125\n",
      "batch [210] loss: 149978.1875\n",
      "batch [220] loss: 148839.453125\n",
      "batch [230] loss: 147691.21875\n",
      "batch [240] loss: 146531.390625\n",
      "batch [250] loss: 145351.109375\n",
      "batch [260] loss: 144150.28125\n",
      "batch [270] loss: 142908.171875\n",
      "batch [280] loss: 141642.203125\n",
      "batch [290] loss: 140360.046875\n",
      "batch [300] loss: 139053.796875\n",
      "batch [310] loss: 137726.078125\n",
      "batch [320] loss: 136361.6875\n",
      "batch [330] loss: 134978.046875\n",
      "batch [340] loss: 133574.09375\n",
      "batch [350] loss: 132129.296875\n",
      "batch [360] loss: 130646.984375\n",
      "batch [370] loss: 129146.65625\n",
      "batch [380] loss: 127628.4921875\n",
      "batch [390] loss: 126078.453125\n",
      "batch [400] loss: 124511.1328125\n",
      "batch [410] loss: 122918.609375\n",
      "batch [420] loss: 121294.2734375\n",
      "batch [430] loss: 119650.2265625\n",
      "batch [440] loss: 117968.28125\n",
      "batch [450] loss: 116259.5078125\n",
      "batch [460] loss: 114525.5859375\n",
      "batch [470] loss: 112769.3984375\n",
      "batch [480] loss: 110984.5546875\n",
      "batch [490] loss: 109172.0859375\n",
      "batch [500] loss: 107343.0703125\n",
      "batch [510] loss: 105485.109375\n",
      "batch [520] loss: 103606.3359375\n",
      "batch [530] loss: 101710.6640625\n",
      "batch [540] loss: 99786.1875\n",
      "batch [550] loss: 97824.8828125\n",
      "batch [560] loss: 95842.15625\n",
      "batch [570] loss: 93840.2890625\n",
      "batch [580] loss: 91825.5546875\n",
      "batch [590] loss: 89802.0859375\n",
      "batch [600] loss: 87755.6875\n",
      "batch [610] loss: 85684.5390625\n",
      "batch [620] loss: 83586.4296875\n",
      "batch [630] loss: 81448.484375\n",
      "batch [640] loss: 79284.4921875\n",
      "batch [650] loss: 77085.71875\n",
      "batch [660] loss: 74845.6328125\n",
      "batch [670] loss: 72585.5234375\n",
      "batch [680] loss: 70308.9140625\n",
      "batch [690] loss: 68018.296875\n",
      "batch [700] loss: 65709.765625\n",
      "batch [710] loss: 63399.59375\n",
      "batch [720] loss: 61089.265625\n",
      "batch [730] loss: 58775.890625\n",
      "batch [740] loss: 56437.203125\n",
      "batch [750] loss: 54062.37890625\n",
      "batch [760] loss: 51650.84765625\n",
      "batch [770] loss: 49189.07421875\n",
      "batch [780] loss: 46693.78125\n",
      "batch [790] loss: 44145.94921875\n",
      "batch [800] loss: 41539.74609375\n",
      "batch [810] loss: 39510.015625\n",
      "batch [820] loss: 37878.265625\n",
      "batch [830] loss: 36233.71484375\n",
      "batch [840] loss: 34578.31640625\n",
      "batch [850] loss: 32877.02734375\n",
      "batch [860] loss: 31143.134765625\n",
      "batch [870] loss: 29382.0390625\n",
      "batch [880] loss: 28346.12109375\n",
      "batch [890] loss: 27636.64453125\n",
      "batch [900] loss: 26914.876953125\n",
      "batch [910] loss: 26177.583984375\n",
      "batch [920] loss: 25426.73046875\n",
      "batch [930] loss: 24672.671875\n",
      "batch [940] loss: 23913.78515625\n",
      "batch [950] loss: 23147.81640625\n",
      "batch [960] loss: 22375.451171875\n",
      "batch [970] loss: 21607.548828125\n",
      "batch [980] loss: 20849.666015625\n",
      "batch [990] loss: 20090.154296875\n",
      "Step 8 Using scale consts: [21.25, 21.25, 21.25, 325.0, 21.25, 21.25, 21.25, 4.9375, 21.25, 21.25, 21.25, 4.9375, 325.0, 2.6875, 21.25, 100000.0, 6.0625, 4.9375, 21.25, 21.25, 4.9375, 21.25, 21.25, 4.9375, 4.9375, 21.25, 21.25, 21.25, 21.25, 1.5625, 4.9375, 21.25, 21.25, 4.9375, 4.9375, 21.25, 1.5625, 21.25, 21.25, 21.25, 21.25, 5500.0, 21.25, 21.25, 21.25, 21.25, 3.90625e-06, 3.8125, 21.25, 4.9375, 1.5625, 21.25, 4.9375, 21.25, 21.25, 21.25, 21.25, 21.25, 21.25, 21.25, 21.25, 1.5625, 21.25, 21.25, 21.25, 4.9375, 5500.0, 4.9375, 2.6875, 21.25, 3.8125, 21.25, 2.6875, 21.25, 21.25, 21.25, 21.25, 21.25, 5500.0, 21.25, 21.25, 3.8125, 21.25, 1.5625, 4.9375, 21.25, 21.25, 21.25, 21.25, 4.9375, 21.25, 4.9375, 21.25, 21.25, 21.25, 21.25, 21.25, 21.25, 21.25, 325.0]\n",
      "batch [0] loss: 136654.578125\n",
      "batch [10] loss: 115123.0546875\n",
      "batch [20] loss: 89160.578125\n",
      "batch [30] loss: 64189.74609375\n",
      "batch [40] loss: 40748.234375\n",
      "batch [50] loss: 19312.955078125\n",
      "batch [60] loss: 6250.89453125\n",
      "batch [70] loss: 6250.4619140625\n",
      "batch [80] loss: 6249.10498046875\n",
      "batch [90] loss: 6247.12744140625\n",
      "batch [100] loss: 6245.107421875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch [110] loss: 6243.0908203125\n",
      "batch [120] loss: 6241.1474609375\n",
      "batch [130] loss: 6239.17138671875\n",
      "batch [140] loss: 6237.05615234375\n",
      "batch [150] loss: 6234.9384765625\n",
      "batch [160] loss: 6232.8857421875\n",
      "batch [170] loss: 6230.8798828125\n",
      "batch [180] loss: 6228.86962890625\n",
      "batch [190] loss: 6226.89404296875\n",
      "batch [200] loss: 6224.7568359375\n",
      "batch [210] loss: 6222.8330078125\n",
      "batch [220] loss: 6220.59814453125\n",
      "batch [230] loss: 6218.6259765625\n",
      "batch [240] loss: 6216.5009765625\n",
      "batch [250] loss: 6214.537109375\n",
      "batch [260] loss: 6212.3447265625\n",
      "batch [270] loss: 6210.263671875\n",
      "batch [280] loss: 6208.2578125\n",
      "batch [290] loss: 6206.37255859375\n",
      "batch [300] loss: 6204.18701171875\n",
      "batch [310] loss: 6202.17822265625\n",
      "batch [320] loss: 6199.87939453125\n",
      "batch [330] loss: 6198.04931640625\n",
      "batch [340] loss: 6195.82373046875\n",
      "batch [350] loss: 6193.6259765625\n",
      "batch [360] loss: 6191.697265625\n",
      "batch [370] loss: 6189.4736328125\n",
      "batch [380] loss: 6187.32861328125\n",
      "batch [390] loss: 6185.2294921875\n",
      "batch [400] loss: 6182.95654296875\n",
      "batch [410] loss: 6180.72802734375\n",
      "batch [420] loss: 6178.6005859375\n",
      "batch [430] loss: 6176.41552734375\n",
      "batch [440] loss: 6174.20361328125\n",
      "batch [450] loss: 6172.02783203125\n",
      "batch [460] loss: 6169.85791015625\n",
      "batch [470] loss: 6167.62255859375\n",
      "batch [480] loss: 6165.40673828125\n",
      "batch [490] loss: 6163.03955078125\n",
      "batch [500] loss: 6160.78125\n",
      "batch [510] loss: 6158.55322265625\n",
      "batch [520] loss: 6156.29443359375\n",
      "batch [530] loss: 6154.09130859375\n",
      "batch [540] loss: 6151.9873046875\n",
      "batch [550] loss: 6149.607421875\n",
      "batch [560] loss: 6147.234375\n",
      "batch [570] loss: 6144.85693359375\n",
      "batch [580] loss: 6142.54345703125\n",
      "batch [590] loss: 6140.28125\n",
      "batch [600] loss: 6137.90234375\n",
      "batch [610] loss: 6135.8076171875\n",
      "batch [620] loss: 6133.1953125\n",
      "batch [630] loss: 6131.01318359375\n",
      "batch [640] loss: 6128.68505859375\n",
      "batch [650] loss: 6126.10693359375\n",
      "batch [660] loss: 6123.74853515625\n",
      "batch [670] loss: 6121.310546875\n",
      "batch [680] loss: 6118.9013671875\n",
      "batch [690] loss: 6116.56396484375\n",
      "batch [700] loss: 6114.11865234375\n",
      "batch [710] loss: 6111.58349609375\n",
      "batch [720] loss: 6109.18994140625\n",
      "batch [730] loss: 6106.72314453125\n",
      "batch [740] loss: 6104.24169921875\n",
      "batch [750] loss: 6101.66455078125\n",
      "batch [760] loss: 6099.18896484375\n",
      "batch [770] loss: 6097.046875\n",
      "batch [780] loss: 6094.20751953125\n",
      "batch [790] loss: 6091.66162109375\n",
      "batch [800] loss: 6089.13720703125\n",
      "batch [810] loss: 6086.5400390625\n",
      "batch [820] loss: 6083.79931640625\n",
      "batch [830] loss: 6081.18798828125\n",
      "batch [840] loss: 6078.9013671875\n",
      "batch [850] loss: 6076.181640625\n",
      "batch [860] loss: 6073.3701171875\n",
      "batch [870] loss: 6070.70654296875\n",
      "batch [880] loss: 6068.248046875\n",
      "batch [890] loss: 6065.4658203125\n",
      "batch [900] loss: 6062.6640625\n",
      "batch [910] loss: 6060.1962890625\n",
      "batch [920] loss: 6057.23583984375\n",
      "batch [930] loss: 6054.54541015625\n",
      "batch [940] loss: 6051.7490234375\n",
      "batch [950] loss: 6049.1611328125\n",
      "batch [960] loss: 6046.10302734375\n",
      "batch [970] loss: 6043.3134765625\n",
      "batch [980] loss: 6040.52392578125\n",
      "batch [990] loss: 6037.8193359375\n",
      "Step 9 Using scale consts: [21.25, 21.25, 21.25, 325.0, 21.25, 21.25, 21.25, 4.9375, 21.25, 21.25, 21.25, 4.9375, 325.0, 3.25, 21.25, 100000.0, 6.0625, 5.5, 21.25, 21.25, 5.5, 21.25, 21.25, 5.5, 4.9375, 21.25, 21.25, 21.25, 21.25, 1.5625, 4.9375, 21.25, 21.25, 4.9375, 4.9375, 21.25, 1.5625, 21.25, 21.25, 21.25, 21.25, 5500.0, 21.25, 21.25, 21.25, 21.25, 3.90625e-06, 3.8125, 21.25, 4.9375, 2.125, 32.5, 5.5, 21.25, 21.25, 21.25, 21.25, 21.25, 21.25, 21.25, 21.25, 2.125, 21.25, 21.25, 21.25, 4.9375, 5500.0, 4.9375, 2.6875, 21.25, 3.8125, 21.25, 3.25, 21.25, 21.25, 21.25, 21.25, 21.25, 5500.0, 21.25, 21.25, 3.8125, 21.25, 2.125, 4.9375, 21.25, 21.25, 21.25, 21.25, 5.5, 21.25, 4.9375, 21.25, 21.25, 21.25, 21.25, 21.25, 21.25, 21.25, 325.0]\n",
      "batch [0] loss: 6042.34228515625\n",
      "batch [10] loss: 6039.2578125\n",
      "batch [20] loss: 6035.62890625\n",
      "batch [30] loss: 6032.078125\n",
      "batch [40] loss: 6028.43798828125\n",
      "batch [50] loss: 6025.1123046875\n",
      "batch [60] loss: 6021.7080078125\n",
      "batch [70] loss: 6018.57666015625\n",
      "batch [80] loss: 6015.55224609375\n",
      "batch [90] loss: 6012.5048828125\n",
      "batch [100] loss: 6009.42919921875\n",
      "batch [110] loss: 6006.3095703125\n",
      "batch [120] loss: 6003.07861328125\n",
      "batch [130] loss: 6000.06005859375\n",
      "batch [140] loss: 5996.93310546875\n",
      "batch [150] loss: 5993.97216796875\n",
      "batch [160] loss: 5990.7685546875\n",
      "batch [170] loss: 5987.60791015625\n",
      "batch [180] loss: 5984.4345703125\n",
      "batch [190] loss: 5981.1865234375\n",
      "batch [200] loss: 5978.13525390625\n",
      "batch [210] loss: 5974.755859375\n",
      "batch [220] loss: 5971.583984375\n",
      "batch [230] loss: 5968.36328125\n",
      "batch [240] loss: 5965.146484375\n",
      "batch [250] loss: 5961.6982421875\n",
      "batch [260] loss: 5958.36083984375\n",
      "batch [270] loss: 5955.24560546875\n",
      "batch [280] loss: 5951.69970703125\n",
      "batch [290] loss: 5948.46142578125\n",
      "batch [300] loss: 5945.07763671875\n",
      "batch [310] loss: 5941.97900390625\n",
      "batch [320] loss: 5938.5205078125\n",
      "batch [330] loss: 5935.15283203125\n",
      "batch [340] loss: 5931.75732421875\n",
      "batch [350] loss: 5928.560546875\n",
      "batch [360] loss: 5924.84716796875\n",
      "batch [370] loss: 5921.38623046875\n",
      "batch [380] loss: 5917.8115234375\n",
      "batch [390] loss: 5914.32275390625\n",
      "batch [400] loss: 5910.7763671875\n",
      "batch [410] loss: 5907.2431640625\n",
      "batch [420] loss: 5903.591796875\n",
      "batch [430] loss: 5900.1396484375\n",
      "batch [440] loss: 5896.826171875\n",
      "batch [450] loss: 5893.27783203125\n",
      "batch [460] loss: 5889.59326171875\n",
      "batch [470] loss: 5886.1640625\n",
      "batch [480] loss: 5882.6611328125\n",
      "batch [490] loss: 5878.970703125\n",
      "batch [500] loss: 5875.4580078125\n",
      "batch [510] loss: 5871.9716796875\n",
      "batch [520] loss: 5868.251953125\n",
      "batch [530] loss: 5864.6494140625\n",
      "batch [540] loss: 5861.1826171875\n",
      "batch [550] loss: 5857.53369140625\n",
      "batch [560] loss: 5854.0087890625\n",
      "batch [570] loss: 5850.4150390625\n",
      "batch [580] loss: 5846.85986328125\n",
      "batch [590] loss: 5843.13134765625\n",
      "batch [600] loss: 5839.552734375\n",
      "batch [610] loss: 5835.9921875\n",
      "batch [620] loss: 5832.599609375\n",
      "batch [630] loss: 5828.91064453125\n",
      "batch [640] loss: 5825.25537109375\n",
      "batch [650] loss: 5821.60107421875\n",
      "batch [660] loss: 5818.3125\n",
      "batch [670] loss: 5814.5087890625\n",
      "batch [680] loss: 5810.93359375\n",
      "batch [690] loss: 5806.93603515625\n",
      "batch [700] loss: 5803.39501953125\n",
      "batch [710] loss: 5799.72900390625\n",
      "batch [720] loss: 5796.01611328125\n",
      "batch [730] loss: 5792.38623046875\n",
      "batch [740] loss: 5788.65087890625\n",
      "batch [750] loss: 5785.123046875\n",
      "batch [760] loss: 5781.3779296875\n",
      "batch [770] loss: 5777.76416015625\n",
      "batch [780] loss: 5773.931640625\n",
      "batch [790] loss: 5770.28271484375\n",
      "batch [800] loss: 5766.5234375\n",
      "batch [810] loss: 5763.02001953125\n",
      "batch [820] loss: 5759.12353515625\n",
      "batch [830] loss: 5755.482421875\n",
      "batch [840] loss: 5751.81884765625\n",
      "batch [850] loss: 5748.1650390625\n",
      "batch [860] loss: 5744.31689453125\n",
      "batch [870] loss: 5740.66162109375\n",
      "batch [880] loss: 5737.076171875\n",
      "batch [890] loss: 5733.40673828125\n",
      "batch [900] loss: 5729.552734375\n",
      "batch [910] loss: 5725.54736328125\n",
      "batch [920] loss: 5722.13330078125\n",
      "batch [930] loss: 5718.126953125\n",
      "batch [940] loss: 5714.3310546875\n",
      "batch [950] loss: 5710.48095703125\n",
      "batch [960] loss: 5706.8046875\n",
      "batch [970] loss: 5702.958984375\n",
      "batch [980] loss: 5699.21533203125\n",
      "batch [990] loss: 5695.37109375\n"
     ]
    }
   ],
   "source": [
    "import cw\n",
    "\n",
    "inputs_box = (min((0 - m) / s for m, s in zip(mean, std)),\n",
    "              max((1 - m) / s for m, s in zip(mean, std)))\n",
    "\n",
    "# an untargeted adversary\n",
    "adversary = cw.L2Adversary(targeted=False,\n",
    "                           confidence=0.0,\n",
    "                           search_steps=10,\n",
    "                           box=inputs_box,\n",
    "                           optimizer_lr=5e-4)\n",
    "\n",
    "inputs, targets = next(iter(dataloader))\n",
    "\n",
    "adversarial_examples = adversary(model, inputs, targets, to_numpy=False)\n",
    "assert isinstance(adversarial_examples, torch.FloatTensor)\n",
    "assert adversarial_examples.size() == inputs.size()\n",
    "\n",
    "# # a targeted adversary\n",
    "# adversary = cw.L2Adversary(targeted=True,\n",
    "#                            confidence=0.0,\n",
    "#                            search_steps=10,\n",
    "#                            box=inputs_box,\n",
    "#                            optimizer_lr=5e-4)\n",
    "\n",
    "# inputs, targets = next(iter(dataloader))\n",
    "# # a batch of any attack targets\n",
    "# attack_targets = torch.ones(inputs.size(0)) * 3 # target is 3\n",
    "# adversarial_examples = adversary(model, inputs, attack_targets, to_numpy=False)\n",
    "# assert isinstance(adversarial_examples, torch.FloatTensor)\n",
    "# assert adversarial_examples.size() == inputs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perturbed Accuracy: 0/100 (0%)\n",
      "\n",
      "Original Accuracy: 99/100 (99%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x576 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pert_output = model(adversarial_examples)\n",
    "orig_output = model(inputs)\n",
    "\n",
    "pert_pred = torch.argmax(pert_output, dim=1)\n",
    "orig_pred = torch.argmax(orig_output, dim=1)\n",
    "\n",
    "pert_correct = pert_pred.eq(targets.data).sum()\n",
    "orig_correct = orig_pred.eq(targets.data).sum()\n",
    "\n",
    "pert_acc = 100. * pert_correct / len(targets)\n",
    "orig_acc = 100. * orig_correct / len(targets)\n",
    "\n",
    "print('Perturbed Accuracy: {}/{} ({:.0f}%)\\n'.format(pert_correct, len(targets), pert_acc))\n",
    "print('Original Accuracy: {}/{} ({:.0f}%)\\n'.format(orig_correct, len(targets), orig_acc))\n",
    "\n",
    "# inputs, adversarial_examples, targets\n",
    "num_samples = 5\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "\n",
    "for i in range(1,num_samples+1):\n",
    "    \n",
    "    plt.subplot(2, num_samples, i)\n",
    "    plt.imshow(np.squeeze(inputs[i].numpy()))  \n",
    "    plt.title('true: {}'.format(targets[i].item()))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    \n",
    "    plt.subplot(2, num_samples, num_samples+i)\n",
    "    plt.imshow(np.squeeze(adversarial_examples[i].numpy()))\n",
    "    plt.title('adv_pred: {} - orig_pred: {}'.format(pert_pred[i].item(), orig_pred[i].item()))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective Function\n",
    "\n",
    "$\\sum_{i=1}^N LOSS[f(x_i+\\delta), y_i] + \\lambda \\cdot DIVERGENCE[f(x_i+\\delta),DISTRIBUTION_{uniform}]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diversity_objective(model, layer, data, target, delta=None, loss_fn=None, regularizer_weight=None):\n",
    "  \n",
    "    if loss_fn is None: \n",
    "        loss_fn = F.cross_entropy # F.nll_loss, \n",
    "    \n",
    "    if regularizer_weight is None:\n",
    "        regularizer_weight = 0.005  \n",
    "\n",
    "    if delta is None:\n",
    "        # get the dimensions of the input, excluding the batch dimension\n",
    "        C, N, M = list(data.size()[1:]) # C = channel, N = row, M = column\n",
    "        delta = torch.rand((C, N, M), requires_grad=True)\n",
    "    else:\n",
    "        # required to track gradients wrt the perturbations to the input\n",
    "        if not delta.requires_grad:\n",
    "            delta.requires_grad = True\n",
    "         \n",
    "    obj = loss_fn(model(data + delta), target) + regularizer_weight * measureKLDivLossAgainstUniform(data + delta, model, layer)\n",
    "  \n",
    "    return obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing area for complete workflow\n",
    "\n",
    "1. Add hook to model at a given layer\n",
    "2. Extract activations from a batch of inputs (NOTE: activations are not binarized (0,1), but actual activation outputs (0,))\n",
    "3. Normalize (softmax them so that they sum to 1) across inputs\n",
    "4. Normalize a second time across neurons so that you have a density that can be reasonably compared against a uniform distribution\n",
    "5. Compute the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
