{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yeR2kRlPR7v0"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import datetime\n",
    "import glob\n",
    "import os\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "IByUFiD8VfFD",
    "outputId": "a04c9ebe-a36a-4104-ef75-5810358b39bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is not available.  Training on CPU ...\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 5\n",
    "batch_size_train = 64\n",
    "batch_size_test = 100\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "log_interval = 100\n",
    "\n",
    "random_seed = 1\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "# check if CUDA is available\n",
    "device = torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    print('CUDA is available!  Training on GPU ...')\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print('CUDA is not available.  Training on CPU ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "7l36L4F5VifU",
    "outputId": "a8882ad0-9b9e-40bc-cb2f-9fa4c30c2795"
   },
   "outputs": [],
   "source": [
    "#  torchvision.transforms.Normalize(\n",
    "#    (0.1307,), (0.3081,))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.MNIST('/data/', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor()\n",
    "                             ])),\n",
    "    batch_size=batch_size_train, shuffle=True, pin_memory=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.MNIST('/data/', train=False, download=True,\n",
    "                         transform=torchvision.transforms.Compose([\n",
    "                           torchvision.transforms.ToTensor()\n",
    "                         ])),\n",
    "    batch_size=batch_size_test, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yKEku0wcSCVQ"
   },
   "outputs": [],
   "source": [
    "class DenseNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DenseNet, self).__init__()\n",
    "        self.dens1 = nn.Linear(784, 256)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.drop1 = nn.Dropout(0.2)\n",
    "        self.dens2 = nn.Linear(256, 128)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.drop2 = nn.Dropout(0.2)\n",
    "        self.dens3 = nn.Linear(128, 64)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.drop3 = nn.Dropout(0.2)\n",
    "        self.dens4 = nn.Linear(64, 20)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.drop4 = nn.Dropout(0.2)\n",
    "        self.dens5 = nn.Linear(20, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dens1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.drop1(x)\n",
    "        x = self.dens2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.drop2(x)\n",
    "        x = self.dens3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.drop3(x)\n",
    "        x = self.dens4(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.drop4(x)\n",
    "        x = self.dens5(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "    def extract_outputs(self, data, layer, neuron=None):\n",
    "        outputs = []      \n",
    "        def hook(module, input, output):\n",
    "            outputs.append(output)    \n",
    "        for name, module in self.named_children():\n",
    "            if name == layer:\n",
    "                handle = module.register_forward_hook(hook)     \n",
    "        out = self(data)\n",
    "        if not neuron is None:\n",
    "            outputs[0] = outputs[0][0][neuron]\n",
    "        else:\n",
    "            outputs[0] = outputs[0][0]\n",
    "        handle.remove()\n",
    "        return torch.stack(outputs)\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(4 * 4 * 50, 500)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4 * 4 * 50)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "    def extract_outputs(self, data, layer, neuron=None):\n",
    "        outputs = []      \n",
    "        def hook(module, input, output):\n",
    "            outputs.append(output)    \n",
    "        for name, module in self.named_children():\n",
    "            if name == layer:\n",
    "                handle = module.register_forward_hook(hook)     \n",
    "        out = self(data)\n",
    "        if not neuron is None:\n",
    "            outputs[0] = outputs[0][0][neuron]\n",
    "        else:\n",
    "            outputs[0] = outputs[0][0]\n",
    "        handle.remove()\n",
    "        return torch.stack(outputs)\n",
    "  \n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        # calculate robust loss\n",
    "        loss = F.cross_entropy(model(data), target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                       100. * batch_idx / len(train_loader), loss.item()))\n",
    "            \n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "YCva0V7uVIZ_",
    "outputId": "f84a9d4a-88b9-4f87-8f82-09ede53f6046"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file models already exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model models\\model_ConvNet_2019-07-09 15.50.26.212191.pth\n"
     ]
    }
   ],
   "source": [
    "model = ConvNet().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "# check to see if we can just load a previous model\n",
    "%mkdir models\n",
    "latest_model = None\n",
    "m_type = model.__class__.__name__\n",
    "prev_models = glob.glob('models/*'+ m_type +'*.pth')\n",
    "if prev_models:\n",
    "    latest_model = max(prev_models, key=os.path.getctime)\n",
    "\n",
    "if latest_model is not None and m_type in latest_model:\n",
    "    print('loading model', latest_model)\n",
    "    model.load_state_dict(torch.load(latest_model))  \n",
    "else:\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        train(model, device, train_loader, optimizer, epoch)\n",
    "        test(model, device, test_loader)    \n",
    "    torch.save(model.state_dict(), 'models/model_' + m_type + '_' +str(datetime.datetime.now()).replace(':','.') + '.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_xXXilwbZq-v"
   },
   "source": [
    "# Helper Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Im4wutR6ge14"
   },
   "outputs": [],
   "source": [
    "def atanh(x, eps=1e-2):\n",
    "    \"\"\"\n",
    "    The inverse hyperbolic tangent function, missing in pytorch.\n",
    "\n",
    "    :param x: a tensor or a Variable\n",
    "    :param eps: used to enhance numeric stability\n",
    "    :return: :math:`\\\\tanh^{-1}{x}`, of the same type as ``x``\n",
    "    \"\"\"\n",
    "    x = x * (1 - eps)\n",
    "    return 0.5 * torch.log((1.0 + x) / (1.0 - x))\n",
    "\n",
    "def to_tanh_space(x, box=(-1., 1.)):\n",
    "    \"\"\"\n",
    "    Convert a batch of tensors to tanh-space. This method complements the\n",
    "    implementation of the change-of-variable trick in terms of tanh.\n",
    "\n",
    "    :param x: the batch of tensors, of dimension [B x C x H x W]\n",
    "    :param box: a tuple of lower bound and upper bound of the box constraint\n",
    "    :return: the batch of tensors in tanh-space, of the same dimension;\n",
    "             the returned tensor is on the same device as ``x``\n",
    "    \"\"\"\n",
    "    _box_mul = (box[1] - box[0]) * 0.5\n",
    "    _box_plus = (box[1] + box[0]) * 0.5\n",
    "    return atanh((x - _box_plus) / _box_mul)\n",
    "\n",
    "def from_tanh_space(x, box=(-1., 1.)):\n",
    "    \"\"\"\n",
    "    Convert a batch of tensors from tanh-space to oridinary image space.\n",
    "    This method complements the implementation of the change-of-variable trick\n",
    "    in terms of tanh.\n",
    "\n",
    "    :param x: the batch of tensors, of dimension [B x C x H x W]\n",
    "    :param box: a tuple of lower bound and upper bound of the box constraint\n",
    "    :return: the batch of tensors in ordinary image space, of the same\n",
    "             dimension; the returned tensor is on the same device as ``x``\n",
    "    \"\"\"\n",
    "    _box_mul = (box[1] - box[0]) * 0.5\n",
    "    _box_plus = (box[1] + box[0]) * 0.5\n",
    "    return torch.tanh(x) * _box_mul + _box_plus\n",
    "  \n",
    "def compensate_confidence(outputs, targets):\n",
    "    \"\"\"\n",
    "    Compensate for ``self.confidence`` and returns a new weighted sum\n",
    "    vector.\n",
    "\n",
    "    :param outputs: the weighted sum right before the last layer softmax\n",
    "           normalization, of dimension [B x M]\n",
    "    :type outputs: np.ndarray\n",
    "    :param targets: either the attack targets or the real image labels,\n",
    "           depending on whether or not ``self.targeted``, of dimension [B]\n",
    "    :type targets: np.ndarray\n",
    "    :return: the compensated weighted sum of dimension [B x M]\n",
    "    :rtype: np.ndarray\n",
    "    \"\"\"\n",
    "    outputs_comp = outputs.clone()\n",
    "    rng = torch.range(start=0, end=targets.shape[0]-1, dtype=torch.long, device=device)\n",
    "    # targets = targets.int()\n",
    "    if targeted:\n",
    "        # for each image $i$:\n",
    "        # if targeted, `outputs[i, target_onehot]` should be larger than\n",
    "        # `max(outputs[i, ~target_onehot])` by `self.confidence`\n",
    "        outputs_comp[rng, targets] -= confidence\n",
    "    else:\n",
    "        # for each image $i$:\n",
    "        # if not targeted, `max(outputs[i, ~target_onehot]` should be larger\n",
    "        # than `outputs[i, target_onehot]` (the ground truth image labels)\n",
    "        # by `self.confidence`\n",
    "        outputs_comp[rng, targets] += confidence\n",
    "    return outputs_comp\n",
    "  \n",
    "def attack_successful(prediction, target):\n",
    "    \"\"\"\n",
    "    See whether the underlying attack is successful.\n",
    "    \"\"\"\n",
    "    if targeted:\n",
    "        return prediction == target\n",
    "    else:\n",
    "        return prediction != target\n",
    "      \n",
    "def norm_divergence(data, model, layer, neuron=None, regularizer_weight=None):\n",
    "    \"\"\"\n",
    "    returns the kld between the activations of the specified layer and a uniform pdf\n",
    "    \"\"\"\n",
    "    # extract layer activations as numpy array\n",
    "    layer_activations = torch.squeeze(model.extract_outputs(data=data, layer=layer))\n",
    "    \n",
    "    # normalize over summation (to get a probability density)\n",
    "    out_norm = torch.sum(layer_activations, 0)\n",
    "    out_norm = out_norm / torch.sum(out_norm) + 1e-6 # F.softmax(out_norm, 1)\n",
    "\n",
    "    # create uniform tensor\n",
    "    uniform_tensor = torch.ones(out_norm.shape).to(device)\n",
    "\n",
    "    # normalize over summation (to get a probability density)\n",
    "    uni_norm = uniform_tensor / torch.sum(uniform_tensor)\n",
    "    \n",
    "    # measure divergence between normalized layer activations and uniform distribution\n",
    "    divergence = F.kl_div(input=out_norm.log(), target=uni_norm, reduction='sum')\n",
    "    \n",
    "    # default regularizer if not provided\n",
    "    if regularizer_weight is None:\n",
    "        regularizer_weight = 0.005 \n",
    "    \n",
    "    return regularizer_weight * divergence\n",
    "\n",
    "def eval_performance(model, originals, adversaries):\n",
    "    pert_output = model(adversaries)\n",
    "    orig_output = model(originals)\n",
    "\n",
    "    pert_pred = torch.argmax(pert_output, dim=1)\n",
    "    orig_pred = torch.argmax(orig_output, dim=1)\n",
    "\n",
    "    pert_correct = pert_pred.eq(targets.data).sum()\n",
    "    orig_correct = orig_pred.eq(targets.data).sum()\n",
    "\n",
    "    pert_acc = 100. * pert_correct / len(targets)\n",
    "    orig_acc = 100. * orig_correct / len(targets)\n",
    "\n",
    "    print('Perturbed Accuracy: {}/{} ({:.0f}%)\\n'.format(pert_correct, len(targets), pert_acc))\n",
    "    print('Original Accuracy: {}/{} ({:.0f}%)\\n'.format(orig_correct, len(targets), orig_acc))\n",
    "    \n",
    "    return pert_acc, orig_acc\n",
    "\n",
    "def sample_images(originals, adversaries, num_samples = 5):\n",
    "    orig_inputs = originals.cpu().detach().numpy()\n",
    "    adv_examples = adversaries.cpu().detach().numpy()\n",
    "    pert_output = model(adversaries)\n",
    "    orig_output = model(originals)\n",
    "    pert_pred = torch.argmax(pert_output, dim=1)\n",
    "    orig_pred = torch.argmax(orig_output, dim=1)\n",
    "    plt.figure(figsize=(15,8))\n",
    "    for i in range(1, num_samples+1):\n",
    "        plt.subplot(2, num_samples, i)\n",
    "        plt.imshow(np.squeeze(orig_inputs[i]), cmap='gray')  \n",
    "        plt.title('true: {}'.format(targets[i].item()))\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "\n",
    "        plt.subplot(2, num_samples, num_samples+i)\n",
    "        plt.imshow(np.squeeze(adv_examples[i]), cmap='gray')\n",
    "        plt.title('adv_pred: {} - orig_pred: {}'.format(pert_pred[i].item(), orig_pred[i].item()))\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X7o_T3B3dWzu"
   },
   "outputs": [],
   "source": [
    "# targets = true labels only for when you're doing a targeted attack\n",
    "# otherwise, you're going to make the inputs easier to classify to \n",
    "# do a targeted attack, targets should be some class other than\n",
    "# the true label\n",
    "\n",
    "inputs, targets = next(iter(test_loader))\n",
    "\n",
    "inputs = inputs.to(device)\n",
    "targets = targets.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "65vIqFkkCkYI"
   },
   "source": [
    "# Attack Time\n",
    "\n",
    "| Version | Loss Function | Scaling Constant | Regularizer | Adversary Selection |\n",
    "| - | - | - | - | - |\n",
    "|  Baseline CW | CW |  True | L2 |  L2 |\n",
    "|  Diversity v1 | CW |  True | Batch Divergence | Instance Divergence |\n",
    "|  Diversity v2 | CW |  False | Batch Divergence | Instance Divergence |\n",
    "|  Diversity v3 | Cross Entropy |  False | Batch Divergence | Instance Divergence |\n",
    "|  Diversity v4 | CW |  True | Batch Divergence | L2 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LlxUuPx1zYE-"
   },
   "source": [
    "## Baseline CW\n",
    "\n",
    "| Loss Function | Scaling Constant | Regularizer | Adversary Selection |\n",
    "| - | - | - | - |\n",
    "|  CW |  True | L2 | L2 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KiGJy0r2h4aJ"
   },
   "outputs": [],
   "source": [
    "def cw_l2_attack(model, inputs, targets, targeted=False, confidence=0.0,\n",
    "                 c_range=(1e-3, 1e10), search_steps=5, max_steps=1000, \n",
    "                 abort_early=True, box=(-1., 1.), optimizer_lr=1e-2, \n",
    "                 init_rand=False, log_frequency=10):\n",
    "\n",
    "    batch_size = inputs.size(0)\n",
    "    num_classes = model(torch.tensor(inputs[0][None,:], requires_grad=False)).size(1)\n",
    "\n",
    "    # `lower_bounds`, `upper_bounds` and `scale_consts` are used\n",
    "    # for binary search of each `scale_const` in the batch. The element-wise\n",
    "    # inquality holds: lower_bounds < scale_consts <= upper_bounds\n",
    "    lower_bounds = torch.tensor(np.zeros(batch_size), dtype=torch.float, device=device)\n",
    "    upper_bounds = torch.tensor(np.ones(batch_size) * c_range[1], dtype=torch.float, device=device)\n",
    "    scale_consts = torch.tensor(np.ones(batch_size) * c_range[0], dtype=torch.float, device=device)\n",
    "\n",
    "    # Optimal attack to be found.\n",
    "    # The three \"placeholders\" are defined as:\n",
    "    # - `o_best_l2`          : the least L2 norms\n",
    "    # - `o_best_l2_ppred`    : the perturbed predictions made by the adversarial perturbations with the least L2 norms\n",
    "    # - `o_best_adversaries` : the underlying adversarial example of `o_best_l2_ppred`\n",
    "    o_best_l2 = torch.tensor(np.ones(batch_size) * np.inf, dtype=torch.float, device=device)\n",
    "    o_best_l2_ppred = torch.tensor(-np.ones(batch_size), dtype=torch.float, device=device)\n",
    "    o_best_adversaries = inputs.clone()\n",
    "\n",
    "    # convert `inputs` to tanh-space\n",
    "    inputs_tanh = to_tanh_space(inputs)\n",
    "    targets_oh = F.one_hot(targets).float()\n",
    "\n",
    "    # the perturbation tensor (only one we need to track gradients on)\n",
    "    pert_tanh = torch.zeros(inputs.size(), device=device, requires_grad=True)\n",
    "\n",
    "    optimizer = optim.Adam([pert_tanh], lr=optimizer_lr)\n",
    "\n",
    "    for const_step in range(search_steps):\n",
    "\n",
    "        print('Step', const_step)\n",
    "\n",
    "        # the minimum L2 norms of perturbations found during optimization\n",
    "        best_l2 = torch.tensor(np.ones(batch_size) * np.inf, dtype=torch.float, device=device)\n",
    "\n",
    "        # the perturbed predictions made by the adversarial perturbations with the least L2 norms\n",
    "        best_l2_ppred = torch.tensor(-np.ones(batch_size), dtype=torch.float, device=device)\n",
    "\n",
    "        # previous (summed) batch loss, to be used in early stopping policy\n",
    "        prev_batch_loss = torch.tensor(np.inf, device=device)\n",
    "        ae_tol = torch.tensor(1e-4, device=device)\n",
    "\n",
    "        # optimization steps\n",
    "        for optim_step in range(max_steps):\n",
    "\n",
    "            adversaries = from_tanh_space(inputs_tanh + pert_tanh)\n",
    "            pert_outputs = model(adversaries)\n",
    "\n",
    "            # Calculate L2 norm between adversaries and original inputs\n",
    "            pert_norms = torch.pow(adversaries - inputs, exponent=2)\n",
    "            pert_norms = torch.sum(pert_norms.view(pert_norms.size(0), -1), 1)\n",
    "\n",
    "            target_activ = torch.sum(targets_oh * pert_outputs, 1)\n",
    "            maxother_activ = torch.max(((1 - targets_oh) * pert_outputs - targets_oh * 1e4), 1)[0]\n",
    "\n",
    "            if targeted:           \n",
    "                # if targeted, optimize to make `target_activ` larger than `maxother_activ` by `confidence`\n",
    "                f = torch.clamp(maxother_activ - target_activ + confidence, min=0.0)\n",
    "            else:\n",
    "                # if not targeted, optimize to make `maxother_activ` larger than `target_activ` (the ground truth image labels) by `confidence`\n",
    "                f = torch.clamp(target_activ - maxother_activ + confidence, min=0.0)\n",
    "\n",
    "            # the total loss of current batch, should be of dimension [1]\n",
    "            batch_loss = torch.sum(pert_norms + scale_consts * f)\n",
    "\n",
    "            # Do optimization for one step\n",
    "            optimizer.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # \"returns\" batch_loss, pert_norms, pert_outputs, adversaries\n",
    "\n",
    "            if optim_step % log_frequency == 0: \n",
    "                print('batch [{}] loss: {}'.format(optim_step, batch_loss))\n",
    "\n",
    "            if abort_early and not optim_step % (max_steps // 10):   \n",
    "                if batch_loss > prev_batch_loss * (1 - ae_tol):\n",
    "                    break\n",
    "                prev_batch_loss = batch_loss\n",
    "\n",
    "            # update best attack found during optimization\n",
    "            pert_predictions = torch.argmax(pert_outputs, dim=1)\n",
    "            comp_pert_predictions = torch.argmax(compensate_confidence(pert_outputs, targets), dim=1)\n",
    "            for i in range(batch_size):\n",
    "                l2 = pert_norms[i]\n",
    "                cppred = comp_pert_predictions[i]\n",
    "                ppred = pert_predictions[i]\n",
    "                tlabel = targets[i]\n",
    "                ax = adversaries[i]\n",
    "                if attack_successful(cppred, tlabel):\n",
    "                    assert cppred == ppred\n",
    "                    if l2 < best_l2[i]:\n",
    "                        best_l2[i] = l2\n",
    "                        best_l2_ppred[i] = ppred\n",
    "                    if l2 < o_best_l2[i]:\n",
    "                        o_best_l2[i] = l2\n",
    "                        o_best_l2_ppred[i] = ppred\n",
    "                        o_best_adversaries[i] = ax\n",
    "\n",
    "        # binary search of `scale_const`\n",
    "        for i in range(batch_size):\n",
    "            tlabel = targets[i]\n",
    "            if best_l2_ppred[i] != -1:\n",
    "                # successful: attempt to lower `scale_const` by halving it\n",
    "                if scale_consts[i] < upper_bounds[i]:\n",
    "                    upper_bounds[i] = scale_consts[i]\n",
    "                # `upper_bounds[i] == c_range[1]` implies no solution\n",
    "                # found, i.e. upper_bounds[i] has never been updated by\n",
    "                # scale_consts[i] until `scale_consts[i] > 0.1 * c_range[1]`\n",
    "                if upper_bounds[i] < c_range[1] * 0.1:\n",
    "                    scale_consts[i] = (lower_bounds[i] + upper_bounds[i]) / 2\n",
    "            else:\n",
    "                # failure: multiply `scale_const` by ten if no solution\n",
    "                # found; otherwise do binary search\n",
    "                if scale_consts[i] > lower_bounds[i]:\n",
    "                    lower_bounds[i] = scale_consts[i]\n",
    "                if upper_bounds[i] < c_range[1] * 0.1:\n",
    "                    scale_consts[i] = (lower_bounds[i] + upper_bounds[i]) / 2\n",
    "                else:\n",
    "                    scale_consts[i] *= 10\n",
    "                    \n",
    "    return o_best_adversaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VbAGYjiGODjF"
   },
   "outputs": [],
   "source": [
    "targeted=False\n",
    "confidence=0.0\n",
    "c_range=(1e-3, 1e10)\n",
    "search_steps=10\n",
    "max_steps=1000\n",
    "abort_early=True\n",
    "optimizer_lr=5e-4\n",
    "\n",
    "mean = (0.1307,) # the mean used in inputs normalization\n",
    "std = (0.3081,) # the standard deviation used in inputs normalization\n",
    "box = (min((0 - m) / s for m, s in zip(mean, std)),\n",
    "       max((1 - m) / s for m, s in zip(mean, std)))\n",
    "\n",
    "log_frequency = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kLueeAz4_-MO"
   },
   "outputs": [],
   "source": [
    "cw_advs = cw_l2_attack(model, inputs, targets, targeted=False, confidence=0.0,\n",
    "                       c_range=(1e-3, 1e10), search_steps=5, max_steps=1000, \n",
    "                       abort_early=True, box=box, optimizer_lr=5e-4, \n",
    "                       init_rand=False, log_frequency=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 315
    },
    "colab_type": "code",
    "id": "CUsvsyleV1Is",
    "outputId": "8bdaf0ca-903b-4871-c249-456f949ae275"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perturbed Accuracy: 0/100 (0%)\n",
      "\n",
      "Original Accuracy: 99/100 (99%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x576 with 10 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_performance(model, inputs, cw_advs)\n",
    "sample_images(inputs, cw_advs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "colab_type": "code",
    "id": "TLhycjrBPl3j",
    "outputId": "080b9c4b-e448-4118-b69b-e7f3ab829ca9"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow((cw_advs[1] - inputs[1]).cpu().detach().numpy().reshape(28,28), cmap='gray') \n",
    "plt.title(targets[1].cpu().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4PMv-KEuX9Q2"
   },
   "source": [
    "## Diversity Attack v1\n",
    "\n",
    "| Loss Function | Scaling Constant | Regularizer | Adversary Selection |\n",
    "| - | - | - | - |\n",
    "|  CW |  True | Batch Divergence | Instance Divergence |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MiwScd1eX1eJ"
   },
   "outputs": [],
   "source": [
    "def cw_div1_attack(model, layer, regularizer_weight, inputs, targets, targeted=False, \n",
    "                   confidence=0.0, c_range=(1e-3, 1e10), search_steps=5, max_steps=1000, \n",
    "                   abort_early=True, box=(-1., 1.), optimizer_lr=1e-2, \n",
    "                   init_rand=False, log_frequency=10):\n",
    "\n",
    "    batch_size = inputs.size(0)\n",
    "    num_classes = model(torch.tensor(inputs[0][None,:], requires_grad=False)).size(1)\n",
    "\n",
    "    # `lower_bounds`, `upper_bounds` and `scale_consts` are used\n",
    "    # for binary search of each `scale_const` in the batch. The element-wise\n",
    "    # inquality holds: lower_bounds < scale_consts <= upper_bounds\n",
    "    lower_bounds = torch.tensor(np.zeros(batch_size), dtype=torch.float, device=device)\n",
    "    upper_bounds = torch.tensor(np.ones(batch_size) * c_range[1], dtype=torch.float, device=device)\n",
    "    scale_consts = torch.tensor(np.ones(batch_size) * c_range[0], dtype=torch.float, device=device)\n",
    "\n",
    "    # Optimal attack to be found.\n",
    "    # The three \"placeholders\" are defined as:\n",
    "    # - `o_best_div`         : the least divergences\n",
    "    # - `o_best_div_ppred`   : the perturbed predictions made by the adversarial perturbations with the least divergences\n",
    "    # - `o_best_adversaries` : the underlying adversarial example of `o_best_div_ppred`\n",
    "    o_best_div = torch.tensor(np.ones(batch_size) * np.inf, dtype=torch.float, device=device)\n",
    "    o_best_div_ppred = torch.tensor(-np.ones(batch_size), dtype=torch.float, device=device)\n",
    "    o_best_adversaries = inputs.clone()\n",
    "\n",
    "    # convert `inputs` to tanh-space\n",
    "    inputs_tanh = to_tanh_space(inputs)\n",
    "    targets_oh = F.one_hot(targets).float()\n",
    "\n",
    "    # the perturbation tensor (only one we need to track gradients on)\n",
    "    pert_tanh = torch.zeros(inputs.size(), device=device, requires_grad=True)\n",
    "\n",
    "    optimizer = optim.Adam([pert_tanh], lr=optimizer_lr)\n",
    "\n",
    "    for const_step in range(search_steps):\n",
    "\n",
    "        print('Step', const_step)\n",
    "\n",
    "        # the minimum divergences of perturbations found during optimization\n",
    "        best_div = torch.tensor(np.ones(batch_size) * np.inf, dtype=torch.float, device=device)\n",
    "\n",
    "        # the perturbed predictions made by the adversarial perturbations with the least divergences\n",
    "        best_div_ppred = torch.tensor(-np.ones(batch_size), dtype=torch.float, device=device)\n",
    "\n",
    "        # previous (summed) batch loss, to be used in early stopping policy\n",
    "        prev_batch_loss = torch.tensor(np.inf, device=device)\n",
    "        ae_tol = torch.tensor(1e-4, device=device)\n",
    "\n",
    "        # optimization steps\n",
    "        for optim_step in range(max_steps):\n",
    "\n",
    "            adversaries = from_tanh_space(inputs_tanh + pert_tanh)\n",
    "            pert_outputs = model(adversaries)\n",
    "\n",
    "            # calculate kl divergence for each input to use for adversary selection\n",
    "            divs = []\n",
    "            for i in range(batch_size):\n",
    "                divs.append(norm_divergence(data=adversaries[i].unsqueeze(0), model=model, layer=layer, regularizer_weight=regularizer_weight)) \n",
    "            div_norms = torch.tensor(torch.stack(divs), device=device)\n",
    "            \n",
    "            # calculate kl divergence for batch to use in loss function\n",
    "            div_reg = norm_divergence(data=adversaries, model=model, layer=layer, regularizer_weight=regularizer_weight)\n",
    "\n",
    "            target_activ = torch.sum(targets_oh * pert_outputs, 1)\n",
    "            maxother_activ = torch.max(((1 - targets_oh) * pert_outputs - targets_oh * 1e4), 1)[0]\n",
    "\n",
    "            if targeted:           \n",
    "                # if targeted, optimize to make `target_activ` larger than `maxother_activ` by `confidence`\n",
    "                f = torch.clamp(maxother_activ - target_activ + confidence, min=0.0)\n",
    "            else:\n",
    "                # if not targeted, optimize to make `maxother_activ` larger than `target_activ` (the ground truth image labels) by `confidence`\n",
    "                f = torch.clamp(target_activ - maxother_activ + confidence, min=0.0)\n",
    "\n",
    "            # the total loss of current batch, should be of dimension [1]\n",
    "            batch_loss = torch.sum(scale_consts * f) - div_reg\n",
    "\n",
    "            # Do optimization for one step\n",
    "            optimizer.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # \"returns\" batch_loss, pert_norms, pert_outputs, adversaries\n",
    "\n",
    "            if optim_step % log_frequency == 0: \n",
    "                print('batch [{}] loss: {}'.format(optim_step, batch_loss))\n",
    "\n",
    "            if abort_early and not optim_step % (max_steps // 10):   \n",
    "                if batch_loss > prev_batch_loss * (1 - ae_tol):\n",
    "                    break\n",
    "                prev_batch_loss = batch_loss\n",
    "\n",
    "            # update best attack found during optimization\n",
    "            pert_predictions = torch.argmax(pert_outputs, dim=1)\n",
    "            comp_pert_predictions = torch.argmax(compensate_confidence(pert_outputs, targets), dim=1)\n",
    "            for i in range(batch_size):\n",
    "                div = div_norms[i]\n",
    "                cppred = comp_pert_predictions[i]\n",
    "                ppred = pert_predictions[i]\n",
    "                tlabel = targets[i]\n",
    "                ax = adversaries[i]\n",
    "                if attack_successful(cppred, tlabel):\n",
    "                    assert cppred == ppred\n",
    "                    if div < best_div[i]:\n",
    "                        best_div[i] = div\n",
    "                        best_div_ppred[i] = ppred\n",
    "                    if div < o_best_div[i]:\n",
    "                        o_best_div[i] = div\n",
    "                        o_best_div_ppred[i] = ppred\n",
    "                        o_best_adversaries[i] = ax\n",
    "\n",
    "        # binary search of `scale_const`\n",
    "        for i in range(batch_size):\n",
    "            tlabel = targets[i]\n",
    "            if best_div_ppred[i] != -1:\n",
    "                # successful: attempt to lower `scale_const` by halving it\n",
    "                if scale_consts[i] < upper_bounds[i]:\n",
    "                    upper_bounds[i] = scale_consts[i]\n",
    "                # `upper_bounds[i] == c_range[1]` implies no solution\n",
    "                # found, i.e. upper_bounds[i] has never been updated by\n",
    "                # scale_consts[i] until `scale_consts[i] > 0.1 * c_range[1]`\n",
    "                if upper_bounds[i] < c_range[1] * 0.1:\n",
    "                    scale_consts[i] = (lower_bounds[i] + upper_bounds[i]) / 2\n",
    "            else:\n",
    "                # failure: multiply `scale_const` by ten if no solution\n",
    "                # found; otherwise do binary search\n",
    "                if scale_consts[i] > lower_bounds[i]:\n",
    "                    lower_bounds[i] = scale_consts[i]\n",
    "                if upper_bounds[i] < c_range[1] * 0.1:\n",
    "                    scale_consts[i] = (lower_bounds[i] + upper_bounds[i]) / 2\n",
    "                else:\n",
    "                    scale_consts[i] *= 10\n",
    "                    \n",
    "    return o_best_adversaries, div_norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "BU2t6ERVB6C3",
    "outputId": "b39a6161-ad16-46c0-a816-72e857fac30b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0\n",
      "batch [0] loss: 0.25219982862472534\n",
      "batch [100] loss: 0.1603238880634308\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-ed45f64a4ae5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m                                           \u001b[0mc_range\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1e-3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1e10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msearch_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                                           \u001b[0mabort_early\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbox\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer_lr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5e-4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m                                           init_rand=False, log_frequency=100)\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0meval_performance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcw_advs_div1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-b4f3afa62d5d>\u001b[0m in \u001b[0;36mcw_div1_attack\u001b[1;34m(model, layer, regularizer_weight, inputs, targets, targeted, confidence, c_range, search_steps, max_steps, abort_early, box, optimizer_lr, init_rand, log_frequency)\u001b[0m\n\u001b[0;32m     55\u001b[0m             \u001b[0mdivs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m                 \u001b[0mdivs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnorm_divergence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0madversaries\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregularizer_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mregularizer_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m             \u001b[0mdiv_norms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdivs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-da2e96d881ab>\u001b[0m in \u001b[0;36mnorm_divergence\u001b[1;34m(data, model, layer, neuron, regularizer_weight)\u001b[0m\n\u001b[0;32m     83\u001b[0m     \"\"\"\n\u001b[0;32m     84\u001b[0m     \u001b[1;31m# extract layer activations as numpy array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m     \u001b[0mlayer_activations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextract_outputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m     \u001b[1;31m# normalize over summation (to get a probability density)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-561528d6c307>\u001b[0m in \u001b[0;36mextract_outputs\u001b[1;34m(self, data, layer, neuron)\u001b[0m\n\u001b[0;32m     79\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m                 \u001b[0mhandle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister_forward_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mneuron\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m             \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mneuron\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-561528d6c307>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextract_outputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneuron\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlog_softmax\u001b[1;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[0;32m   1348\u001b[0m         \u001b[0mdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'log_softmax'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1350\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1351\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cw_advs_div1, div_norms1 = cw_div1_attack(model, 'relu3', 1, inputs, targets, targeted=False, confidence=0.0,\n",
    "                                          c_range=(1e-3, 1e10), search_steps=1, max_steps=1000, \n",
    "                                          abort_early=True, box=box, optimizer_lr=5e-4, \n",
    "                                          init_rand=False, log_frequency=100)\n",
    "\n",
    "eval_performance(model, inputs, cw_advs_div1)\n",
    "sample_images(inputs, cw_advs_div1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WzVUYdHrRczQ"
   },
   "source": [
    "## Diversity Attack v2\n",
    "\n",
    "| Loss Function | Scaling Constant | Regularizer | Adversary Selection |\n",
    "| - | - | - | - |\n",
    "|  CW |  False | Batch Divergence | Instance Divergence |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KRG0FJayRLyi"
   },
   "outputs": [],
   "source": [
    "def cw_div2_attack(model, layer, regularizer_weight, inputs, targets, targeted=False, \n",
    "                   confidence=0.0, c_range=(1e-3, 1e10), search_steps=5, max_steps=1000, \n",
    "                   abort_early=True, box=(-1., 1.), optimizer_lr=1e-2, \n",
    "                   init_rand=False, log_frequency=10):\n",
    "\n",
    "    batch_size = inputs.size(0)\n",
    "    num_classes = model(torch.tensor(inputs[0][None,:], requires_grad=False)).size(1)\n",
    "\n",
    "    # Optimal attack to be found.\n",
    "    # The three \"placeholders\" are defined as:\n",
    "    # - `o_best_div`         : the least divergences\n",
    "    # - `o_best_div_ppred`   : the perturbed predictions made by the adversarial perturbations with the least divergences\n",
    "    # - `o_best_adversaries` : the underlying adversarial example of `o_best_div_ppred`\n",
    "    o_best_div = torch.tensor(np.ones(batch_size) * np.inf, dtype=torch.float, device=device)\n",
    "    o_best_div_ppred = torch.tensor(-np.ones(batch_size), dtype=torch.float, device=device)\n",
    "    o_best_adversaries = inputs.clone()\n",
    "\n",
    "    # convert `inputs` to tanh-space\n",
    "    inputs_tanh = to_tanh_space(inputs)\n",
    "    targets_oh = F.one_hot(targets).float()\n",
    "\n",
    "    # the perturbation tensor (only one we need to track gradients on)\n",
    "    pert_tanh = torch.zeros(inputs.size(), device=device, requires_grad=True)\n",
    "\n",
    "    optimizer = optim.Adam([pert_tanh], lr=optimizer_lr)\n",
    "\n",
    "    # the minimum divergences of perturbations found during optimization\n",
    "    best_div = torch.tensor(np.ones(batch_size) * np.inf, dtype=torch.float, device=device)\n",
    "\n",
    "    # the perturbed predictions made by the adversarial perturbations with the least divergences\n",
    "    best_div_ppred = torch.tensor(-np.ones(batch_size), dtype=torch.float, device=device)\n",
    "\n",
    "    # previous (summed) batch loss, to be used in early stopping policy\n",
    "    prev_batch_loss = torch.tensor(np.inf, device=device)\n",
    "    ae_tol = torch.tensor(1e-4, device=device)\n",
    "\n",
    "    # optimization steps\n",
    "    for optim_step in range(max_steps):\n",
    "\n",
    "        adversaries = from_tanh_space(inputs_tanh + pert_tanh)\n",
    "        pert_outputs = model(adversaries)\n",
    "\n",
    "        # calculate kl divergence for each input to use for adversary selection\n",
    "        divs = []\n",
    "        for i in range(batch_size):\n",
    "            divs.append(norm_divergence(data=adversaries[i].unsqueeze(0), model=model, layer=layer, regularizer_weight=regularizer_weight)) \n",
    "        div_norms = torch.tensor(torch.stack(divs), device=device)\n",
    "\n",
    "        # calculate kl divergence for batch to use in loss function\n",
    "        div_reg = norm_divergence(data=adversaries, model=model, layer=layer, regularizer_weight=regularizer_weight)\n",
    "\n",
    "        target_activ = torch.sum(targets_oh * pert_outputs, 1)\n",
    "        maxother_activ = torch.max(((1 - targets_oh) * pert_outputs - targets_oh * 1e4), 1)[0]\n",
    "\n",
    "        if targeted:           \n",
    "            # if targeted, optimize to make `target_activ` larger than `maxother_activ` by `confidence`\n",
    "            f = torch.clamp(maxother_activ - target_activ + confidence, min=0.0)\n",
    "        else:\n",
    "            # if not targeted, optimize to make `maxother_activ` larger than `target_activ` (the ground truth image labels) by `confidence`\n",
    "            f = torch.clamp(target_activ - maxother_activ + confidence, min=0.0)\n",
    "\n",
    "        # the total loss of current batch, should be of dimension [1]\n",
    "        batch_loss = torch.sum(f) + div_reg\n",
    "\n",
    "        # Do optimization for one step\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # \"returns\" batch_loss, pert_norms, pert_outputs, adversaries\n",
    "\n",
    "        if optim_step % log_frequency == 0: \n",
    "            print('batch [{}] loss: {}'.format(optim_step, batch_loss))\n",
    "\n",
    "        if abort_early and not optim_step % (max_steps // 10):   \n",
    "            if batch_loss > prev_batch_loss * (1 - ae_tol):\n",
    "                break\n",
    "            prev_batch_loss = batch_loss\n",
    "\n",
    "        # update best attack found during optimization\n",
    "        pert_predictions = torch.argmax(pert_outputs, dim=1)\n",
    "        comp_pert_predictions = torch.argmax(compensate_confidence(pert_outputs, targets), dim=1)\n",
    "        for i in range(batch_size):\n",
    "            div = div_norms[i]\n",
    "            cppred = comp_pert_predictions[i]\n",
    "            ppred = pert_predictions[i]\n",
    "            tlabel = targets[i]\n",
    "            ax = adversaries[i]\n",
    "            if attack_successful(cppred, tlabel):\n",
    "                assert cppred == ppred\n",
    "                if div < best_div[i]:\n",
    "                    best_div[i] = div\n",
    "                    best_div_ppred[i] = ppred\n",
    "                if div < o_best_div[i]:\n",
    "                    o_best_div[i] = div\n",
    "                    o_best_div_ppred[i] = ppred\n",
    "                    o_best_adversaries[i] = ax\n",
    "                    \n",
    "    return o_best_adversaries, div_norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j6KVf6RRRLyn"
   },
   "outputs": [],
   "source": [
    "cw_advs_div2, div_norms2 = cw_div2_attack(model, 'relu3', 1, inputs, targets, targeted=False, confidence=0.0,\n",
    "                                          c_range=(1e-3, 1e10), search_steps=1, max_steps=1000, \n",
    "                                          abort_early=True, box=box, optimizer_lr=5e-4, \n",
    "                                          init_rand=False, log_frequency=100)\n",
    "\n",
    "eval_performance(model, inputs, cw_advs_div2)\n",
    "sample_images(inputs, cw_advs_div2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "63AbLZogRLzE",
    "outputId": "fd26755d-7682-4321-accd-171aed7a187e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 59,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cw_advs.equal(cw_advs_div)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cJRhdV5aRLzP",
    "outputId": "9329d136-81a8-4119-e5b8-bfd0fc01c4c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 60,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cw_advs_div.equal(cw_advs_div2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c8SHpsaSRLzX",
    "outputId": "eb3bdd88-5e1f-4640-b082-7a32525ced89"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 61,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cw_advs.equal(cw_advs_div2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ivE3YOQIRLzh",
    "outputId": "b050b669-f87f-47c4-f1c4-86b963baf7cd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3130, -0.3130, -0.3087,  ...,  0.9999,  0.9999,  0.9999],\n",
       "       device='cuda:0', grad_fn=<NotImplemented>)"
      ]
     },
     "execution_count": 80,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cw_advs.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZzFWidecRLzo",
    "outputId": "734dc209-457d-4b33-accf-922713592e21"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4568, -0.4521, -0.4480,  ...,  0.9941,  0.9941,  0.9943],\n",
       "       device='cuda:0', grad_fn=<NotImplemented>)"
      ]
     },
     "execution_count": 79,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cw_advs_div.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t97ieYCkRLzz"
   },
   "source": [
    "## Diversity Attack v3\n",
    "\n",
    "| Loss Function | Scaling Constant | Regularizer | Adversary Selection |\n",
    "| - | - | - | - |\n",
    "| Cross Entropy |  False | Batch Divergence | Instance Divergence |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fuEKLdemRLz1"
   },
   "outputs": [],
   "source": [
    "def cw_div3_attack(model, layer, regularizer_weight, inputs, targets, targeted=False, \n",
    "                   confidence=0.0, c_range=(1e-3, 1e10), search_steps=5, max_steps=1000, \n",
    "                   abort_early=True, box=(-1., 1.), optimizer_lr=1e-2, \n",
    "                   init_rand=False, log_frequency=10):\n",
    "\n",
    "    batch_size = inputs.size(0)\n",
    "    num_classes = model(torch.tensor(inputs[0][None,:], requires_grad=False)).size(1)\n",
    "\n",
    "    # Optimal attack to be found.\n",
    "    # The three \"placeholders\" are defined as:\n",
    "    # - `o_best_div`         : the least divergences\n",
    "    # - `o_best_div_ppred`   : the perturbed predictions made by the adversarial perturbations with the least divergences\n",
    "    # - `o_best_adversaries` : the underlying adversarial example of `o_best_div_ppred`\n",
    "    o_best_div = torch.tensor(np.ones(batch_size) * np.inf, dtype=torch.float, device=device)\n",
    "    o_best_div_ppred = torch.tensor(-np.ones(batch_size), dtype=torch.float, device=device)\n",
    "    o_best_adversaries = inputs.clone()\n",
    "\n",
    "    # convert `inputs` to tanh-space\n",
    "    inputs_tanh = to_tanh_space(inputs)\n",
    "    targets_oh = F.one_hot(targets).float()\n",
    "\n",
    "    # the perturbation tensor (only one we need to track gradients on)\n",
    "    pert_tanh = torch.zeros(inputs.size(), device=device, requires_grad=True)\n",
    "\n",
    "    optimizer = optim.Adam([pert_tanh], lr=optimizer_lr)\n",
    "\n",
    "    # the minimum divergences of perturbations found during optimization\n",
    "    best_div = torch.tensor(np.ones(batch_size) * np.inf, dtype=torch.float, device=device)\n",
    "\n",
    "    # the perturbed predictions made by the adversarial perturbations with the least divergences\n",
    "    best_div_ppred = torch.tensor(-np.ones(batch_size), dtype=torch.float, device=device)\n",
    "\n",
    "    # previous (summed) batch loss, to be used in early stopping policy\n",
    "    prev_batch_loss = torch.tensor(np.inf, device=device)\n",
    "    ae_tol = torch.tensor(1e-4, device=device)\n",
    "\n",
    "    # optimization steps\n",
    "    for optim_step in range(max_steps):\n",
    "\n",
    "        adversaries = from_tanh_space(inputs_tanh + pert_tanh)\n",
    "        pert_outputs = model(adversaries)\n",
    "\n",
    "        # calculate kl divergence for each input to use for adversary selection\n",
    "        divs = []\n",
    "        for i in range(batch_size):\n",
    "            divs.append(norm_divergence(data=adversaries[i].unsqueeze(0), model=model, layer=layer, regularizer_weight=regularizer_weight)) \n",
    "        div_norms = torch.tensor(torch.stack(divs), device=device)\n",
    "\n",
    "        # calculate kl divergence for batch to use in loss function\n",
    "        div_reg = norm_divergence(data=adversaries, model=model, layer=layer, regularizer_weight=regularizer_weight)\n",
    "\n",
    "        loss = -1. * nn.CrossEntropyLoss()(pert_outputs, targets)\n",
    "\n",
    "        # the total loss of current batch, should be of dimension [1]\n",
    "        batch_loss = torch.sum(loss) + div_reg\n",
    "\n",
    "        # Do optimization for one step\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # \"returns\" batch_loss, pert_norms, pert_outputs, adversaries\n",
    "\n",
    "        if optim_step % log_frequency == 0: \n",
    "            print('batch [{}] loss: {}'.format(optim_step, batch_loss))\n",
    "\n",
    "        if abort_early and not optim_step % (max_steps // 10):   \n",
    "            if batch_loss > prev_batch_loss * (1 - ae_tol):\n",
    "                break\n",
    "            prev_batch_loss = batch_loss\n",
    "\n",
    "        # update best attack found during optimization\n",
    "        pert_predictions = torch.argmax(pert_outputs, dim=1)\n",
    "        comp_pert_predictions = torch.argmax(compensate_confidence(pert_outputs, targets), dim=1)\n",
    "        for i in range(batch_size):\n",
    "            div = div_norms[i]\n",
    "            cppred = comp_pert_predictions[i]\n",
    "            ppred = pert_predictions[i]\n",
    "            tlabel = targets[i]\n",
    "            ax = adversaries[i]\n",
    "            if attack_successful(cppred, tlabel):    \n",
    "                assert cppred == ppred\n",
    "                if div < best_div[i]:\n",
    "                    best_div[i] = div\n",
    "                    best_div_ppred[i] = ppred\n",
    "                if div < o_best_div[i]:\n",
    "                    o_best_div[i] = div\n",
    "                    o_best_div_ppred[i] = ppred\n",
    "                    o_best_adversaries[i] = ax\n",
    "                    \n",
    "    return o_best_adversaries, div_norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xqEO2jx8RLz8"
   },
   "outputs": [],
   "source": [
    "cw_advs_div3, div_norms3 = cw_div3_attack(model, 'relu3', 1, inputs, targets, targeted=False, confidence=0.0,\n",
    "                                          c_range=(1e-3, 1e10), search_steps=1, max_steps=1000, \n",
    "                                          abort_early=True, box=box, optimizer_lr=5e-4, \n",
    "                                          init_rand=False, log_frequency=100)\n",
    "\n",
    "eval_performance(model, inputs, cw_advs_div3)\n",
    "sample_images(inputs, cw_advs_div3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6PF994rizIz7"
   },
   "source": [
    "## Diversity Attack v4\n",
    "\n",
    "| Loss Function | Scaling Constant | Regularizer | Adversary Selection |\n",
    "| - | - | - | - |\n",
    "| CW |  True | Batch Divergence | L2 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ulKTYiPhzbjv"
   },
   "outputs": [],
   "source": [
    "def cw_div4_attack(model, layer, regularizer_weight, inputs, targets, targeted=False, \n",
    "                   confidence=0.0, c_range=(1e-3, 1e10), search_steps=5, max_steps=1000, \n",
    "                   abort_early=True, box=(-1., 1.), optimizer_lr=1e-2, \n",
    "                   init_rand=False, log_frequency=10):\n",
    "\n",
    "    batch_size = inputs.size(0)\n",
    "    num_classes = model(torch.tensor(inputs[0][None,:], requires_grad=False)).size(1)\n",
    "\n",
    "    # `lower_bounds`, `upper_bounds` and `scale_consts` are used\n",
    "    # for binary search of each `scale_const` in the batch. The element-wise\n",
    "    # inquality holds: lower_bounds < scale_consts <= upper_bounds\n",
    "    lower_bounds = torch.tensor(np.zeros(batch_size), dtype=torch.float, device=device)\n",
    "    upper_bounds = torch.tensor(np.ones(batch_size) * c_range[1], dtype=torch.float, device=device)\n",
    "    scale_consts = torch.tensor(np.ones(batch_size) * c_range[0], dtype=torch.float, device=device)\n",
    "\n",
    "    # Optimal attack to be found.\n",
    "    # The three \"placeholders\" are defined as:\n",
    "    # - `o_best_l2`          : the least L2 norms\n",
    "    # - `o_best_l2_ppred`    : the perturbed predictions made by the adversarial perturbations with the least L2 norms\n",
    "    # - `o_best_adversaries` : the underlying adversarial example of `o_best_l2_ppred`\n",
    "    o_best_l2 = torch.tensor(np.ones(batch_size) * np.inf, dtype=torch.float, device=device)\n",
    "    o_best_l2_ppred = torch.tensor(-np.ones(batch_size), dtype=torch.float, device=device)\n",
    "    o_best_adversaries = inputs.clone()\n",
    "\n",
    "    # convert `inputs` to tanh-space\n",
    "    inputs_tanh = to_tanh_space(inputs)\n",
    "    targets_oh = F.one_hot(targets).float()\n",
    "\n",
    "    # the perturbation tensor (only one we need to track gradients on)\n",
    "    pert_tanh = torch.zeros(inputs.size(), device=device, requires_grad=True)\n",
    "\n",
    "    optimizer = optim.Adam([pert_tanh], lr=optimizer_lr)\n",
    "\n",
    "    for const_step in range(search_steps):\n",
    "\n",
    "        print('Step', const_step)\n",
    "\n",
    "        # the minimum L2 norms of perturbations found during optimization\n",
    "        best_l2 = torch.tensor(np.ones(batch_size) * np.inf, dtype=torch.float, device=device)\n",
    "\n",
    "        # the perturbed predictions made by the adversarial perturbations with the least L2 norms\n",
    "        best_l2_ppred = torch.tensor(-np.ones(batch_size), dtype=torch.float, device=device)\n",
    "\n",
    "        # previous (summed) batch loss, to be used in early stopping policy\n",
    "        prev_batch_loss = torch.tensor(np.inf, device=device)\n",
    "        ae_tol = torch.tensor(1e-4, device=device) # abort early tolerance\n",
    "\n",
    "        # optimization steps\n",
    "        for optim_step in range(max_steps):\n",
    "\n",
    "            adversaries = from_tanh_space(inputs_tanh + pert_tanh)\n",
    "            pert_outputs = model(adversaries)\n",
    "\n",
    "            # Calculate L2 norm between adversaries and original inputs to use for adversary selection\n",
    "            pert_norms = torch.pow(adversaries - inputs, exponent=2)\n",
    "            pert_norms = torch.sum(pert_norms.view(pert_norms.size(0), -1), 1)\n",
    "            \n",
    "            # calculate kl divergence for batch to use in loss function\n",
    "            div_reg = norm_divergence(data=adversaries, model=model, layer=layer, regularizer_weight=regularizer_weight)\n",
    "\n",
    "            target_activ = torch.sum(targets_oh * pert_outputs, 1)\n",
    "            maxother_activ = torch.max(((1 - targets_oh) * pert_outputs - targets_oh * 1e4), 1)[0]\n",
    "\n",
    "            if targeted:           \n",
    "                # if targeted, optimize to make `target_activ` larger than `maxother_activ` by `confidence`\n",
    "                f = torch.clamp(maxother_activ - target_activ + confidence, min=0.0)\n",
    "            else:\n",
    "                # if not targeted, optimize to make `maxother_activ` larger than `target_activ` (the ground truth image labels) by `confidence`\n",
    "                f = torch.clamp(target_activ - maxother_activ + confidence, min=0.0)\n",
    "\n",
    "            # the total loss of current batch, should be of dimension [1]\n",
    "            batch_loss = torch.sum(scale_consts * f) + div_reg\n",
    "\n",
    "            # Do optimization for one step\n",
    "            optimizer.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # \"returns\" batch_loss, pert_norms, pert_outputs, adversaries\n",
    "\n",
    "            if optim_step % log_frequency == 0: \n",
    "                print('batch [{}] loss: {}'.format(optim_step, batch_loss))\n",
    "\n",
    "            if abort_early and not optim_step % (max_steps // 10):\n",
    "                if batch_loss > prev_batch_loss * (1 - ae_tol):\n",
    "                    break\n",
    "                prev_batch_loss = batch_loss\n",
    "\n",
    "            # update best attack found during optimization\n",
    "            pert_predictions = torch.argmax(pert_outputs, dim=1)\n",
    "            comp_pert_predictions = torch.argmax(compensate_confidence(pert_outputs, targets), dim=1)\n",
    "            for i in range(batch_size):\n",
    "                l2 = pert_norms[i]\n",
    "                cppred = comp_pert_predictions[i]\n",
    "                ppred = pert_predictions[i]\n",
    "                tlabel = targets[i]\n",
    "                ax = adversaries[i]\n",
    "                if attack_successful(cppred, tlabel):\n",
    "                    assert cppred == ppred\n",
    "                    if l2 < best_l2[i]:\n",
    "                        best_l2[i] = l2\n",
    "                        best_l2_ppred[i] = ppred\n",
    "                    if l2 < o_best_l2[i]:\n",
    "                        o_best_l2[i] = l2\n",
    "                        o_best_l2_ppred[i] = ppred\n",
    "                        o_best_adversaries[i] = ax\n",
    "\n",
    "        # binary search of `scale_const`\n",
    "        for i in range(batch_size):\n",
    "            tlabel = targets[i]\n",
    "            if best_l2_ppred[i] != -1:\n",
    "                # successful: attempt to lower `scale_const` by halving it\n",
    "                if scale_consts[i] < upper_bounds[i]:\n",
    "                    upper_bounds[i] = scale_consts[i]\n",
    "                # `upper_bounds[i] == c_range[1]` implies no solution\n",
    "                # found, i.e. upper_bounds[i] has never been updated by\n",
    "                # scale_consts[i] until `scale_consts[i] > 0.1 * c_range[1]`\n",
    "                if upper_bounds[i] < c_range[1] * 0.1:\n",
    "                    scale_consts[i] = (lower_bounds[i] + upper_bounds[i]) / 2\n",
    "            else:\n",
    "                # failure: multiply `scale_const` by ten if no solution\n",
    "                # found; otherwise do binary search\n",
    "                if scale_consts[i] > lower_bounds[i]:\n",
    "                    lower_bounds[i] = scale_consts[i]\n",
    "                if upper_bounds[i] < c_range[1] * 0.1:\n",
    "                    scale_consts[i] = (lower_bounds[i] + upper_bounds[i]) / 2\n",
    "                else:\n",
    "                    scale_consts[i] *= 10\n",
    "                    \n",
    "    return o_best_adversaries, l2_norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aYp4r_BN1wM7"
   },
   "outputs": [],
   "source": [
    "cw_advs_div4, l2_norms4 = cw_div4_attack(model, 'relu3', 1, inputs, targets, targeted=False, confidence=0.,\n",
    "                                         c_range=(1e-3, 1e10), search_steps=1, max_steps=1000, \n",
    "                                         abort_early=True, box=box, optimizer_lr=5e-4, \n",
    "                                         init_rand=False, log_frequency=100)\n",
    "\n",
    "eval_performance(model, inputs, cw_advs_div4)\n",
    "sample_images(inputs, cw_advs_div4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mo0_HOLTdnKe"
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "colab_type": "code",
    "id": "aKgn1boNRL0X",
    "outputId": "936c38b7-a0c3-4048-cd6c-bd5b57426d80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp 2019-07-21 19.50.30.310520 attack cw_div1_attack layer:  relu1 regularization_weight:  0\n",
      "Step 0\n",
      "batch [0] loss: 0.7629573345184326\n",
      "batch [100] loss: 0.5195173025131226\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-92fa10f90ded>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m                                              \u001b[0mc_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearch_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                                              \u001b[0mabort_early\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5e-4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                                              init_rand=False, log_frequency=100)\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m           \u001b[0mpert_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_performance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcw_advs_divs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-46-b4f3afa62d5d>\u001b[0m in \u001b[0;36mcw_div1_attack\u001b[0;34m(model, layer, regularizer_weight, inputs, targets, targeted, confidence, c_range, search_steps, max_steps, abort_early, box, optimizer_lr, init_rand, log_frequency)\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mdivs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m                 \u001b[0mdivs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_divergence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madversaries\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregularizer_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mregularizer_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m             \u001b[0mdiv_norms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdivs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-16d9de95fd8c>\u001b[0m in \u001b[0;36mnorm_divergence\u001b[0;34m(data, model, layer, neuron, regularizer_weight)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \"\"\"\n\u001b[1;32m     91\u001b[0m     \u001b[0;31m# extract layer activations as numpy array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0mlayer_activations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;31m# normalize over summation (to get a probability density)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-561528d6c307>\u001b[0m in \u001b[0;36mextract_outputs\u001b[0;34m(self, data, layer, neuron)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_forward_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mneuron\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mneuron\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-561528d6c307>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m4\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "all_layers = list(dict(model.named_children()))\n",
    "target_layers = [layer for layer in all_layers if 'relu' in layer]\n",
    "attack_versions = [cw_div1_attack, cw_div2_attack, cw_div3_attack, cw_div4_attack]\n",
    "\n",
    "for attack in attack_versions:\n",
    "  for l in target_layers:\n",
    "      for rw in [0, 0.001, 0.01, 1, 10, 100]:\n",
    "          timestamp = str(datetime.datetime.now()).replace(':','.')\n",
    "          print('timestamp', timestamp, 'attack', attack.__name__, 'layer: ', l, 'regularization_weight: ', rw)\n",
    "          cw_advs_divs, divergences = attack(model, l, rw, inputs, targets, targeted=False, confidence=0.0,\n",
    "                                             c_range=(1e-3, 1e10), search_steps=5, max_steps=1000, \n",
    "                                             abort_early=True, box=box, optimizer_lr=5e-4, \n",
    "                                             init_rand=False, log_frequency=100)\n",
    "\n",
    "          pert_acc, orig_acc = eval_performance(model, inputs, cw_advs_divs)\n",
    "          sample_images(inputs, cw_advs_divs)\n",
    "\n",
    "          out = {'timestamp': timestamp, 'attack': attack.__name__, 'layer': l, 'regularization_weight': rw, 'adversaries': cw_advs_divs, 'divergences':divergences, 'pert_acc':pert_acc, 'orig_acc': orig_acc}\n",
    "          results.append(out)\n",
    "        \n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vt1vvsHPe6Vj"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CW_div_pytorch.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
