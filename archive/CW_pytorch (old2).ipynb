{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yeR2kRlPR7v0"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import datetime\n",
    "import glob\n",
    "import os\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "IByUFiD8VfFD",
    "outputId": "a63c70fc-c50f-4d4d-f1d2-3c3a0fbe032e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!  Training on GPU ...\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 5\n",
    "batch_size_train = 64\n",
    "batch_size_test = 100\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "log_interval = 100\n",
    "\n",
    "random_seed = 1\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "# check if CUDA is available\n",
    "device = torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    print('CUDA is available!  Training on GPU ...')\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print('CUDA is not available.  Training on CPU ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "7l36L4F5VifU",
    "outputId": "48441b74-ba8f-4596-e8c2-0b85f97b0897"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9920512it [00:01, 8202852.74it/s]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/28881 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to /data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32768it [00:00, 134594.17it/s]           \n",
      "  0%|          | 0/1648877 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to /data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1654784it [00:00, 2189621.67it/s]                            \n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /data/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8192it [00:00, 51399.03it/s]            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#  torchvision.transforms.Normalize(\n",
    "#    (0.1307,), (0.3081,))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.MNIST('/data/', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor()\n",
    "                             ])),\n",
    "    batch_size=batch_size_train, shuffle=True, pin_memory=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.MNIST('/data/', train=False, download=True,\n",
    "                         transform=torchvision.transforms.Compose([\n",
    "                           torchvision.transforms.ToTensor()\n",
    "                         ])),\n",
    "    batch_size=batch_size_test, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yKEku0wcSCVQ"
   },
   "outputs": [],
   "source": [
    "class DenseNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DenseNet, self).__init__()\n",
    "        self.dens1 = nn.Linear(784, 256)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.drop1 = nn.Dropout(0.2)\n",
    "        self.dens2 = nn.Linear(256, 128)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.drop2 = nn.Dropout(0.2)\n",
    "        self.dens3 = nn.Linear(128, 64)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.drop3 = nn.Dropout(0.2)\n",
    "        self.dens4 = nn.Linear(64, 20)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.drop4 = nn.Dropout(0.2)\n",
    "        self.dens5 = nn.Linear(20, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dens1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.drop1(x)\n",
    "        x = self.dens2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.drop2(x)\n",
    "        x = self.dens3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.drop3(x)\n",
    "        x = self.dens4(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.drop4(x)\n",
    "        x = self.dens5(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "    def extract_outputs(self, data, layer, neuron=None):\n",
    "        outputs = []      \n",
    "        def hook(module, input, output):\n",
    "            outputs.append(output)    \n",
    "        for name, module in self.named_children():\n",
    "            if name == layer:\n",
    "                handle = module.register_forward_hook(hook)     \n",
    "        out = self(data)\n",
    "        if not neuron is None:\n",
    "            outputs[0] = outputs[0][0][neuron]\n",
    "        else:\n",
    "            outputs[0] = outputs[0][0]\n",
    "        handle.remove()\n",
    "        return torch.stack(outputs)\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(4 * 4 * 50, 500)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4 * 4 * 50)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "    def extract_outputs(self, data, layer, neuron=None):\n",
    "        outputs = []      \n",
    "        def hook(module, input, output):\n",
    "            outputs.append(output)    \n",
    "        for name, module in self.named_children():\n",
    "            if name == layer:\n",
    "                handle = module.register_forward_hook(hook)     \n",
    "        out = self(data)\n",
    "        if not neuron is None:\n",
    "            outputs[0] = outputs[0][0][neuron]\n",
    "        else:\n",
    "            outputs[0] = outputs[0][0]\n",
    "        handle.remove()\n",
    "        return torch.stack(outputs)\n",
    "  \n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        # calculate robust loss\n",
    "        loss = F.cross_entropy(model(data), target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                       100. * batch_idx / len(train_loader), loss.item()))\n",
    "            \n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "YCva0V7uVIZ_",
    "outputId": "39b209a1-4aa3-4b70-982e-a9f7c078fcc1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘models’: File exists\n",
      "loading model /content/models/model_ConvNet_2019-07-10 01.15.48.709325.pth\n"
     ]
    }
   ],
   "source": [
    "model = ConvNet().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "# check to see if we can just load a previous model\n",
    "%mkdir models\n",
    "latest_model = None\n",
    "m_type = model.__class__.__name__\n",
    "prev_models = glob.glob('/content/models/*'+ m_type +'*.pth')\n",
    "if prev_models:\n",
    "    latest_model = max(prev_models, key=os.path.getctime)\n",
    "\n",
    "if latest_model is not None and m_type in latest_model:\n",
    "    print('loading model', latest_model)\n",
    "    model.load_state_dict(torch.load(latest_model))  \n",
    "else:\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        train(model, device, train_loader, optimizer, epoch)\n",
    "        test(model, device, test_loader)    \n",
    "    torch.save(model.state_dict(), '/content/models/model_' + m_type + '_' +str(datetime.datetime.now()).replace(':','.') + '.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_xXXilwbZq-v"
   },
   "source": [
    "# Attack Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VbAGYjiGODjF"
   },
   "outputs": [],
   "source": [
    "targeted=False\n",
    "confidence=0.0\n",
    "c_range=(1e-3, 1e10)\n",
    "search_steps=10\n",
    "max_steps=1000\n",
    "abort_early=True\n",
    "optimizer_lr=5e-4\n",
    "\n",
    "mean = (0.1307,) # the mean used in inputs normalization\n",
    "std = (0.3081,) # the standard deviation used in inputs normalization\n",
    "box = (min((0 - m) / s for m, s in zip(mean, std)),\n",
    "       max((1 - m) / s for m, s in zip(mean, std)))\n",
    "\n",
    "log_frequency = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Im4wutR6ge14"
   },
   "outputs": [],
   "source": [
    "def atanh(x, eps=1e-2):\n",
    "    \"\"\"\n",
    "    The inverse hyperbolic tangent function, missing in pytorch.\n",
    "\n",
    "    :param x: a tensor or a Variable\n",
    "    :param eps: used to enhance numeric stability\n",
    "    :return: :math:`\\\\tanh^{-1}{x}`, of the same type as ``x``\n",
    "    \"\"\"\n",
    "    x = x * (1 - eps)\n",
    "    return 0.5 * torch.log((1.0 + x) / (1.0 - x))\n",
    "\n",
    "def to_tanh_space(x, box=box):\n",
    "    \"\"\"\n",
    "    Convert a batch of tensors to tanh-space. This method complements the\n",
    "    implementation of the change-of-variable trick in terms of tanh.\n",
    "\n",
    "    :param x: the batch of tensors, of dimension [B x C x H x W]\n",
    "    :param box: a tuple of lower bound and upper bound of the box constraint\n",
    "    :return: the batch of tensors in tanh-space, of the same dimension;\n",
    "             the returned tensor is on the same device as ``x``\n",
    "    \"\"\"\n",
    "    _box_mul = (box[1] - box[0]) * 0.5\n",
    "    _box_plus = (box[1] + box[0]) * 0.5\n",
    "    return atanh((x - _box_plus) / _box_mul)\n",
    "\n",
    "def from_tanh_space(x, box=box):\n",
    "    \"\"\"\n",
    "    Convert a batch of tensors from tanh-space to oridinary image space.\n",
    "    This method complements the implementation of the change-of-variable trick\n",
    "    in terms of tanh.\n",
    "\n",
    "    :param x: the batch of tensors, of dimension [B x C x H x W]\n",
    "    :param box: a tuple of lower bound and upper bound of the box constraint\n",
    "    :return: the batch of tensors in ordinary image space, of the same\n",
    "             dimension; the returned tensor is on the same device as ``x``\n",
    "    \"\"\"\n",
    "    _box_mul = (box[1] - box[0]) * 0.5\n",
    "    _box_plus = (box[1] + box[0]) * 0.5\n",
    "    return torch.tanh(x) * _box_mul + _box_plus\n",
    "  \n",
    "def compensate_confidence(outputs, targets):\n",
    "    \"\"\"\n",
    "    Compensate for ``self.confidence`` and returns a new weighted sum\n",
    "    vector.\n",
    "\n",
    "    :param outputs: the weighted sum right before the last layer softmax\n",
    "           normalization, of dimension [B x M]\n",
    "    :type outputs: np.ndarray\n",
    "    :param targets: either the attack targets or the real image labels,\n",
    "           depending on whether or not ``self.targeted``, of dimension [B]\n",
    "    :type targets: np.ndarray\n",
    "    :return: the compensated weighted sum of dimension [B x M]\n",
    "    :rtype: np.ndarray\n",
    "    \"\"\"\n",
    "    outputs_comp = outputs.clone()\n",
    "    rng = torch.range(start=0, end=targets.shape[0]-1, dtype=torch.long, device=device)\n",
    "    # targets = targets.int()\n",
    "    if targeted:\n",
    "        # for each image $i$:\n",
    "        # if targeted, `outputs[i, target_onehot]` should be larger than\n",
    "        # `max(outputs[i, ~target_onehot])` by `self.confidence`\n",
    "        outputs_comp[rng, targets] -= confidence\n",
    "    else:\n",
    "        # for each image $i$:\n",
    "        # if not targeted, `max(outputs[i, ~target_onehot]` should be larger\n",
    "        # than `outputs[i, target_onehot]` (the ground truth image labels)\n",
    "        # by `self.confidence`\n",
    "        outputs_comp[rng, targets] += confidence\n",
    "    return outputs_comp\n",
    "  \n",
    "def attack_successful(prediction, target):\n",
    "    \"\"\"\n",
    "    See whether the underlying attack is successful.\n",
    "\n",
    "    :param prediction: the prediction of the model on an input\n",
    "    :type prediction: int\n",
    "    :param target: either the attack target or the ground-truth image label\n",
    "    :type target: int\n",
    "    :return: ``True`` if the attack is successful\n",
    "    :rtype: bool\n",
    "    \"\"\"\n",
    "    if targeted:\n",
    "        return prediction == target\n",
    "    else:\n",
    "        return prediction != target\n",
    "      \n",
    "def norm_divergence(data, model, layer, neuron=None, regularizer_weight=None):\n",
    "    \"\"\"\n",
    "    returns the kld between the activations of the specified layer and a uniform pdf\n",
    "    \"\"\"\n",
    "    # extract layer activations as numpy array\n",
    "    layer_activations = model.extract_outputs(data=data, layer=layer)\n",
    "    \n",
    "    # normalize with softmax (to get a probability density)\n",
    "    out_norm = torch.sum(layer_activations, 0)\n",
    "    out_norm = F.softmax(out_norm, 0)\n",
    "\n",
    "    # create uniform tensor\n",
    "    uniform_tensor = torch.FloatTensor(*out_norm.shape).uniform_(0., 1.).to(device)\n",
    "\n",
    "    # normalize over summation (to get a probability density)\n",
    "    uni_norm = uniform_tensor/torch.sum(uniform_tensor)\n",
    "    \n",
    "    # measure divergence between normalized layer activations and uniform distribution\n",
    "    divergence = F.kl_div(input=out_norm.log(), target=uni_norm, reduction='sum')\n",
    "    \n",
    "    # default regularizer if not provided\n",
    "    if regularizer_weight is None:\n",
    "        regularizer_weight = 0.005 \n",
    "    \n",
    "    return regularizer_weight * divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X7o_T3B3dWzu"
   },
   "outputs": [],
   "source": [
    "# targets = true labels only for when you're doing a targeted attack\n",
    "# otherwise, you're going to make the inputs easier to classify to \n",
    "# do a targeted attack, targets should be some class other than\n",
    "# the true label\n",
    "\n",
    "inputs, targets = next(iter(test_loader))\n",
    "\n",
    "inputs = inputs.to(device)\n",
    "targets = targets.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 663
    },
    "colab_type": "code",
    "id": "KiGJy0r2h4aJ",
    "outputId": "3af67ee9-7ecd-4154-bb98-0176e3d8bf70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0\n",
      "batch [0] loss: 3.0003368854522705\n",
      "batch [100] loss: 2.605081796646118\n",
      "batch [200] loss: 2.287353992462158\n",
      "batch [300] loss: 2.1267948150634766\n",
      "batch [400] loss: 2.172154426574707\n",
      "Step 1\n",
      "batch [0] loss: 2.2897543907165527\n",
      "batch [100] loss: 2.1303482055664062\n",
      "batch [200] loss: 2.2486698627471924\n",
      "Step 2\n",
      "batch [0] loss: 2.1545491218566895\n",
      "batch [100] loss: 2.1900172233581543\n",
      "Step 3\n",
      "batch [0] loss: 2.2388274669647217\n",
      "batch [100] loss: 2.218778133392334\n",
      "batch [200] loss: 2.159066677093506\n",
      "batch [300] loss: 2.2391371726989746\n",
      "Step 4\n",
      "batch [0] loss: 2.171940326690674\n",
      "batch [100] loss: 2.1454687118530273\n",
      "batch [200] loss: 2.233276128768921\n",
      "Step 5\n",
      "batch [0] loss: 2.142425298690796\n",
      "batch [100] loss: 2.153149127960205\n",
      "Step 6\n",
      "batch [0] loss: 2.2117769718170166\n",
      "batch [100] loss: 2.212939739227295\n",
      "Step 7\n",
      "batch [0] loss: 2.181347370147705\n",
      "batch [100] loss: 2.257772207260132\n",
      "Step 8\n",
      "batch [0] loss: 2.1404008865356445\n",
      "batch [100] loss: 2.1429331302642822\n",
      "Step 9\n",
      "batch [0] loss: 2.213601589202881\n",
      "batch [100] loss: 2.1318349838256836\n",
      "batch [200] loss: 2.188809871673584\n"
     ]
    }
   ],
   "source": [
    "batch_size = inputs.size(0)\n",
    "num_classes = model(torch.tensor(inputs[0][None,:], requires_grad=False)).size(1)\n",
    "\n",
    "# `lower_bounds`, `upper_bounds` and `scale_consts` are used\n",
    "# for binary search of each `scale_const` in the batch. The element-wise\n",
    "# inquality holds: lower_bounds < scale_consts <= upper_bounds\n",
    "lower_bounds = torch.tensor(np.zeros(batch_size), dtype=torch.float, device=device)\n",
    "upper_bounds = torch.tensor(np.ones(batch_size) * c_range[1], dtype=torch.float, device=device)\n",
    "scale_consts = torch.tensor(np.ones(batch_size) * c_range[0], dtype=torch.float, device=device)\n",
    "\n",
    "# Optimal attack to be found.\n",
    "# The three \"placeholders\" are defined as:\n",
    "# - `o_best_l2`          : the least L2 norms\n",
    "# - `o_best_l2_ppred`    : the perturbed predictions made by the adversarial perturbations with the least L2 norms\n",
    "# - `o_best_adversaries` : the underlying adversarial example of `o_best_l2_ppred`\n",
    "o_best_l2 = torch.tensor(np.ones(batch_size) * np.inf, dtype=torch.float, device=device)\n",
    "o_best_l2_ppred = torch.tensor(-np.ones(batch_size), dtype=torch.float, device=device)\n",
    "o_best_adversaries = inputs.clone()\n",
    "\n",
    "# convert `inputs` to tanh-space\n",
    "inputs_tanh = to_tanh_space(inputs)\n",
    "targets_oh = F.one_hot(targets).float()\n",
    "\n",
    "# the perturbation tensor (only one we need to track gradients on)\n",
    "pert_tanh = torch.zeros(inputs.size(), device=device, requires_grad=True)\n",
    "\n",
    "optimizer = optim.Adam([pert_tanh], lr=optimizer_lr)\n",
    "\n",
    "for const_step in range(search_steps):\n",
    "  \n",
    "    print('Step', const_step)\n",
    "    \n",
    "    # the minimum L2 norms of perturbations found during optimization\n",
    "    best_l2 = torch.tensor(np.ones(batch_size) * np.inf, dtype=torch.float, device=device)\n",
    "    \n",
    "    # the perturbed predictions made by the adversarial perturbations with the least L2 norms\n",
    "    best_l2_ppred = torch.tensor(-np.ones(batch_size), dtype=torch.float, device=device)\n",
    "    \n",
    "    # previous (summed) batch loss, to be used in early stopping policy\n",
    "    prev_batch_loss = torch.tensor(np.inf, device=device)\n",
    "    ae_tol = torch.tensor(1e-4, device=device)\n",
    "    \n",
    "    # optimization steps\n",
    "    for optim_step in range(max_steps):\n",
    "        \n",
    "        adversaries = from_tanh_space(inputs_tanh + pert_tanh)\n",
    "        pert_outputs = model(adversaries)\n",
    "\n",
    "        # Calculate L2 norm between adversaries and original inputs\n",
    "        pert_norms = torch.pow(adversaries - inputs, exponent=2)\n",
    "        pert_norms = torch.sum(pert_norms.view(pert_norms.size(0), -1), 1)\n",
    "\n",
    "        target_activ = torch.sum(targets_oh * pert_outputs, 1)\n",
    "        maxother_activ = torch.max(((1 - targets_oh) * pert_outputs - targets_oh * 1e4), 1)[0]\n",
    "\n",
    "        if targeted:           \n",
    "            # if targeted, optimize to make `target_activ` larger than `maxother_activ` by `confidence`\n",
    "            f = torch.clamp(maxother_activ - target_activ + confidence, min=0.0)\n",
    "        else:\n",
    "            # if not targeted, optimize to make `maxother_activ` larger than `target_activ` (the ground truth image labels) by `confidence`\n",
    "            f = torch.clamp(target_activ - maxother_activ + confidence, min=0.0)\n",
    "\n",
    "        # the total loss of current batch, should be of dimension [1]\n",
    "        batch_loss = torch.sum(pert_norms + scale_consts * f)\n",
    "\n",
    "        # Do optimization for one step\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # \"returns\" batch_loss, pert_norms, pert_outputs, adversaries\n",
    "\n",
    "        if optim_step % log_frequency == 0: \n",
    "            print('batch [{}] loss: {}'.format(optim_step, batch_loss))\n",
    "\n",
    "        if abort_early and not optim_step % (max_steps // 10):   \n",
    "            if batch_loss > prev_batch_loss * (1 - ae_tol):\n",
    "                break\n",
    "            prev_batch_loss = batch_loss\n",
    "\n",
    "        # update best attack found during optimization\n",
    "        pert_predictions = torch.argmax(pert_outputs, dim=1)\n",
    "        comp_pert_predictions = torch.argmax(compensate_confidence(pert_outputs, targets), dim=1)\n",
    "        for i in range(batch_size):\n",
    "            l2 = pert_norms[i]\n",
    "            cppred = comp_pert_predictions[i]\n",
    "            ppred = pert_predictions[i]\n",
    "            tlabel = targets[i]\n",
    "            ax = adversaries[i]\n",
    "            if attack_successful(cppred, tlabel):\n",
    "                assert cppred == ppred\n",
    "                if l2 < best_l2[i]:\n",
    "                    best_l2[i] = l2\n",
    "                    best_l2_ppred[i] = ppred\n",
    "                if l2 < o_best_l2[i]:\n",
    "                    o_best_l2[i] = l2\n",
    "                    o_best_l2_ppred[i] = ppred\n",
    "                    o_best_adversaries[i] = ax\n",
    "                    \n",
    "    # binary search of `scale_const`\n",
    "    for i in range(batch_size):\n",
    "        tlabel = targets[i]\n",
    "        if best_l2_ppred[i] != -1:\n",
    "            # successful: attempt to lower `scale_const` by halving it\n",
    "            if scale_consts[i] < upper_bounds[i]:\n",
    "                upper_bounds[i] = scale_consts[i]\n",
    "            # `upper_bounds[i] == c_range[1]` implies no solution\n",
    "            # found, i.e. upper_bounds[i] has never been updated by\n",
    "            # scale_consts[i] until `scale_consts[i] > 0.1 * c_range[1]`\n",
    "            if upper_bounds[i] < c_range[1] * 0.1:\n",
    "                scale_consts[i] = (lower_bounds[i] + upper_bounds[i]) / 2\n",
    "        else:\n",
    "            # failure: multiply `scale_const` by ten if no solution\n",
    "            # found; otherwise do binary search\n",
    "            if scale_consts[i] > lower_bounds[i]:\n",
    "                lower_bounds[i] = scale_consts[i]\n",
    "            if upper_bounds[i] < c_range[1] * 0.1:\n",
    "                scale_consts[i] = (lower_bounds[i] + upper_bounds[i]) / 2\n",
    "            else:\n",
    "                scale_consts[i] *= 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "colab_type": "code",
    "id": "CUsvsyleV1Is",
    "outputId": "6e33ab75-9c8c-45c1-a0e3-b7ea3593c65b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perturbed Accuracy: 0/100 (0%)\n",
      "\n",
      "Original Accuracy: 99/100 (99%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAADkCAYAAADNX7BjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pert_output = model(o_best_adversaries)\n",
    "orig_output = model(inputs)\n",
    "\n",
    "pert_pred = torch.argmax(pert_output, dim=1)\n",
    "orig_pred = torch.argmax(orig_output, dim=1)\n",
    "\n",
    "pert_correct = pert_pred.eq(targets.data).sum()\n",
    "orig_correct = orig_pred.eq(targets.data).sum()\n",
    "\n",
    "pert_acc = 100. * pert_correct / len(targets)\n",
    "orig_acc = 100. * orig_correct / len(targets)\n",
    "\n",
    "print('Perturbed Accuracy: {}/{} ({:.0f}%)\\n'.format(pert_correct, len(targets), pert_acc))\n",
    "print('Original Accuracy: {}/{} ({:.0f}%)\\n'.format(orig_correct, len(targets), orig_acc))\n",
    "\n",
    "adversarial_examples = o_best_adversaries.cpu().detach().numpy()\n",
    "input_examples = inputs.cpu().detach().numpy()\n",
    "\n",
    "# inputs, adversarial_examples, targets\n",
    "num_samples = 5\n",
    "\n",
    "for i in range(1,num_samples+1):\n",
    "    \n",
    "    plt.subplot(2, num_samples, i)\n",
    "    plt.imshow(np.squeeze(input_examples[i]), cmap='gray')  \n",
    "    plt.title('actual {}'.format(targets[i].item()))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    \n",
    "    plt.subplot(2, num_samples, num_samples+i)\n",
    "    plt.imshow(np.squeeze(adversarial_examples[i]), cmap='gray')\n",
    "    plt.title('{} {}'.format(pert_pred[i].item(), orig_pred[i].item()))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "colab_type": "code",
    "id": "TLhycjrBPl3j",
    "outputId": "080b9c4b-e448-4118-b69b-e7f3ab829ca9"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEICAYAAACQ6CLfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ng7fzKbjcAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow((o_best_adversaries[1] - inputs[1]).cpu().detach().numpy().reshape(28,28), cmap='gray') \n",
    "plt.title(targets[1].cpu().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4PMv-KEuX9Q2"
   },
   "source": [
    "# Diversity Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 544
    },
    "colab_type": "code",
    "id": "MiwScd1eX1eJ",
    "outputId": "83a4b685-5316-47a5-d57a-16c256c4fb00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0\n",
      "batch [0] loss: 137.5483856201172\n",
      "batch [100] loss: 125.37621307373047\n",
      "batch [200] loss: 116.92552947998047\n",
      "batch [300] loss: 113.6111068725586\n",
      "batch [400] loss: 112.81069946289062\n",
      "batch [500] loss: 113.54646301269531\n",
      "Step 1\n",
      "batch [0] loss: 113.57208251953125\n",
      "batch [100] loss: 113.06755828857422\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-151-2f0b11d877c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mdivs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mdivs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_divergence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madversaries\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu3'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregularizer_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mdiv_norms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdivs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-142-2609299d2cf2>\u001b[0m in \u001b[0;36mnorm_divergence\u001b[0;34m(data, model, layer, neuron, regularizer_weight)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \"\"\"\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# extract layer activations as numpy array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mlayer_activations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# normalize with softmax (to get a probability density)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-75-561528d6c307>\u001b[0m in \u001b[0;36mextract_outputs\u001b[0;34m(self, data, layer, neuron)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_forward_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mneuron\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mneuron\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-75-561528d6c307>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    336\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    337\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 338\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 11.17 GiB total capacity; 10.79 GiB already allocated; 576.00 KiB free; 48.99 MiB cached)"
     ]
    }
   ],
   "source": [
    "batch_size = inputs.size(0)\n",
    "num_classes = model(torch.tensor(inputs[0][None,:], requires_grad=False)).size(1)\n",
    "\n",
    "# `lower_bounds`, `upper_bounds` and `scale_consts` are used\n",
    "# for binary search of each `scale_const` in the batch. The element-wise\n",
    "# inquality holds: lower_bounds < scale_consts <= upper_bounds\n",
    "lower_bounds = torch.tensor(np.zeros(batch_size), dtype=torch.float, device=device)\n",
    "upper_bounds = torch.tensor(np.ones(batch_size) * c_range[1], dtype=torch.float, device=device)\n",
    "scale_consts = torch.tensor(np.ones(batch_size) * c_range[0], dtype=torch.float, device=device)\n",
    "\n",
    "# Optimal attack to be found.\n",
    "# The three \"placeholders\" are defined as:\n",
    "# - `o_best_div`         : the least divergences\n",
    "# - `o_best_div_ppred`   : the perturbed predictions made by the adversarial perturbations with the least divergences\n",
    "# - `o_best_adversaries` : the underlying adversarial example of `o_best_div_ppred`\n",
    "o_best_div = torch.tensor(np.ones(batch_size) * np.inf, dtype=torch.float, device=device)\n",
    "o_best_div_ppred = torch.tensor(-np.ones(batch_size), dtype=torch.float, device=device)\n",
    "o_best_adversaries = inputs.clone()\n",
    "\n",
    "# convert `inputs` to tanh-space\n",
    "inputs_tanh = to_tanh_space(inputs)\n",
    "targets_oh = F.one_hot(targets).float()\n",
    "\n",
    "# the perturbation tensor (only one we need to track gradients on)\n",
    "pert_tanh = torch.zeros(inputs.size(), device=device, requires_grad=True)\n",
    "\n",
    "optimizer = optim.Adam([pert_tanh], lr=optimizer_lr)\n",
    "\n",
    "for const_step in range(search_steps):\n",
    "  \n",
    "    print('Step', const_step)\n",
    "    \n",
    "    # the minimum divergences of perturbations found during optimization\n",
    "    best_div = torch.tensor(np.ones(batch_size) * np.inf, dtype=torch.float, device=device)\n",
    "    \n",
    "    # the perturbed predictions made by the adversarial perturbations with the least divergences\n",
    "    best_div_ppred = torch.tensor(-np.ones(batch_size), dtype=torch.float, device=device)\n",
    "    \n",
    "    # previous (summed) batch loss, to be used in early stopping policy\n",
    "    prev_batch_loss = torch.tensor(np.inf, device=device)\n",
    "    ae_tol = torch.tensor(1e-4, device=device)\n",
    "    \n",
    "    # optimization steps\n",
    "    for optim_step in range(max_steps):\n",
    "        \n",
    "        adversaries = from_tanh_space(inputs_tanh + pert_tanh)\n",
    "        pert_outputs = model(adversaries)\n",
    "        \n",
    "        # calculate kl divergence for each input\n",
    "        divs = []\n",
    "        for i in range(batch_size):\n",
    "            divs.append(norm_divergence(data=adversaries[i].unsqueeze(0), model=model, layer='relu3', regularizer_weight=1))\n",
    "            \n",
    "        div_norms = torch.tensor(torch.stack(divs), device=device)\n",
    "        \n",
    "        target_activ = torch.sum(targets_oh * pert_outputs, 1)\n",
    "        maxother_activ = torch.max(((1 - targets_oh) * pert_outputs - targets_oh * 1e4), 1)[0]\n",
    "\n",
    "        if targeted:           \n",
    "            # if targeted, optimize to make `target_activ` larger than `maxother_activ` by `confidence`\n",
    "            f = torch.clamp(maxother_activ - target_activ + confidence, min=0.0)\n",
    "        else:\n",
    "            # if not targeted, optimize to make `maxother_activ` larger than `target_activ` (the ground truth image labels) by `confidence`\n",
    "            f = torch.clamp(target_activ - maxother_activ + confidence, min=0.0)\n",
    "            \n",
    "#         # diversity regularizer\n",
    "#         diversity_reg = norm_divergence(data=adversaries, model=model, layer='relu3', regularizer_weight=1)\n",
    "   \n",
    "        # the total loss of current batch, should be of dimension [1]\n",
    "        batch_loss = torch.sum(scale_consts * f + div_norms) # + diversity_reg\n",
    "\n",
    "        # Do optimization for one step\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # \"returns\" batch_loss, pert_norms, pert_outputs, adversaries\n",
    "\n",
    "        if optim_step % log_frequency == 0: \n",
    "            print('batch [{}] loss: {}'.format(optim_step, batch_loss))\n",
    "\n",
    "        if abort_early and not optim_step % (max_steps // 10):   \n",
    "            if batch_loss > prev_batch_loss * (1 - ae_tol):\n",
    "                break\n",
    "            prev_batch_loss = batch_loss\n",
    "\n",
    "        # update best attack found during optimization\n",
    "        pert_predictions = torch.argmax(pert_outputs, dim=1)\n",
    "        comp_pert_predictions = torch.argmax(compensate_confidence(pert_outputs, targets), dim=1)\n",
    "        for i in range(batch_size):\n",
    "            div = div_norms[i]\n",
    "            cppred = comp_pert_predictions[i]\n",
    "            ppred = pert_predictions[i]\n",
    "            tlabel = targets[i]\n",
    "            ax = adversaries[i]\n",
    "            if attack_successful(cppred, tlabel):\n",
    "                assert cppred == ppred\n",
    "                if div < best_div[i]:\n",
    "                    best_div[i] = l2\n",
    "                    best_div_ppred[i] = ppred\n",
    "                if div < o_best_div[i]:\n",
    "                    o_best_div[i] = l2\n",
    "                    o_best_div_ppred[i] = ppred\n",
    "                    o_best_adversaries[i] = ax\n",
    "                    \n",
    "    # binary search of `scale_const`\n",
    "    for i in range(batch_size):\n",
    "        tlabel = targets[i]\n",
    "        if best_div_ppred[i] != -1:\n",
    "            # successful: attempt to lower `scale_const` by halving it\n",
    "            if scale_consts[i] < upper_bounds[i]:\n",
    "                upper_bounds[i] = scale_consts[i]\n",
    "            # `upper_bounds[i] == c_range[1]` implies no solution\n",
    "            # found, i.e. upper_bounds[i] has never been updated by\n",
    "            # scale_consts[i] until `scale_consts[i] > 0.1 * c_range[1]`\n",
    "            if upper_bounds[i] < c_range[1] * 0.1:\n",
    "                scale_consts[i] = (lower_bounds[i] + upper_bounds[i]) / 2\n",
    "        else:\n",
    "            # failure: multiply `scale_const` by ten if no solution\n",
    "            # found; otherwise do binary search\n",
    "            if scale_consts[i] > lower_bounds[i]:\n",
    "                lower_bounds[i] = scale_consts[i]\n",
    "            if upper_bounds[i] < c_range[1] * 0.1:\n",
    "                scale_consts[i] = (lower_bounds[i] + upper_bounds[i]) / 2\n",
    "            else:\n",
    "                scale_consts[i] *= 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c651LtJEbi56"
   },
   "outputs": [],
   "source": [
    "pert_output = model(o_best_adversaries)\n",
    "orig_output = model(inputs)\n",
    "\n",
    "pert_pred = torch.argmax(pert_output, dim=1)\n",
    "orig_pred = torch.argmax(orig_output, dim=1)\n",
    "\n",
    "pert_correct = pert_pred.eq(targets.data).sum()\n",
    "orig_correct = orig_pred.eq(targets.data).sum()\n",
    "\n",
    "pert_acc = 100. * pert_correct / len(targets)\n",
    "orig_acc = 100. * orig_correct / len(targets)\n",
    "\n",
    "print('Perturbed Accuracy: {}/{} ({:.0f}%)\\n'.format(pert_correct, len(targets), pert_acc))\n",
    "print('Original Accuracy: {}/{} ({:.0f}%)\\n'.format(orig_correct, len(targets), orig_acc))\n",
    "\n",
    "adversarial_examples = o_best_adversaries.cpu().detach().numpy()\n",
    "input_examples = inputs.cpu().detach().numpy()\n",
    "\n",
    "# inputs, adversarial_examples, targets\n",
    "num_samples = 5\n",
    "\n",
    "for i in range(1,num_samples+1):\n",
    "    \n",
    "    plt.subplot(2, num_samples, i)\n",
    "    plt.imshow(np.squeeze(input_examples[i]), cmap='gray')  \n",
    "    plt.title('actual {}'.format(targets[i].item()))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    \n",
    "    plt.subplot(2, num_samples, num_samples+i)\n",
    "    plt.imshow(np.squeeze(adversarial_examples[i]), cmap='gray')\n",
    "    plt.title('{} {}'.format(pert_pred[i].item(), orig_pred[i].item()))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RcyIPslXb0IJ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CW - pytorch.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
