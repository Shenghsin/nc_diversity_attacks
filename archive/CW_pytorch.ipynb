{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yeR2kRlPR7v0"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import datetime\n",
    "import glob\n",
    "import os\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "IByUFiD8VfFD",
    "outputId": "51056d02-a6c3-4a76-d527-30d7d1b7dcfd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!  Training on GPU ...\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 5\n",
    "batch_size_train = 64\n",
    "batch_size_test = 100\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "log_interval = 100\n",
    "\n",
    "random_seed = 1\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "# check if CUDA is available\n",
    "device = torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    print('CUDA is available!  Training on GPU ...')\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print('CUDA is not available.  Training on CPU ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "colab_type": "code",
    "id": "7l36L4F5VifU",
    "outputId": "d47e9685-ced9-4dfa-ab9d-ef1fa1e2abd7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9920512it [00:02, 4822131.04it/s]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/28881 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to /data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32768it [00:00, 141885.80it/s]           \n",
      "  0%|          | 0/1648877 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to /data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1654784it [00:00, 2321112.49it/s]                            \n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /data/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8192it [00:00, 51793.32it/s]            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#  torchvision.transforms.Normalize(\n",
    "#    (0.1307,), (0.3081,))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.MNIST('/data/', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor()\n",
    "                             ])),\n",
    "    batch_size=batch_size_train, shuffle=True, pin_memory=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.MNIST('/data/', train=False, download=True,\n",
    "                         transform=torchvision.transforms.Compose([\n",
    "                           torchvision.transforms.ToTensor()\n",
    "                         ])),\n",
    "    batch_size=batch_size_test, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yKEku0wcSCVQ"
   },
   "outputs": [],
   "source": [
    "class DenseNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DenseNet, self).__init__()\n",
    "        self.dens1 = nn.Linear(784, 256)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.drop1 = nn.Dropout(0.2)\n",
    "        self.dens2 = nn.Linear(256, 128)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.drop2 = nn.Dropout(0.2)\n",
    "        self.dens3 = nn.Linear(128, 64)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.drop3 = nn.Dropout(0.2)\n",
    "        self.dens4 = nn.Linear(64, 20)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.drop4 = nn.Dropout(0.2)\n",
    "        self.dens5 = nn.Linear(20, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dens1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.drop1(x)\n",
    "        x = self.dens2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.drop2(x)\n",
    "        x = self.dens3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.drop3(x)\n",
    "        x = self.dens4(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.drop4(x)\n",
    "        x = self.dens5(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "    def extract_outputs(self, data, layer, neuron=None):\n",
    "        outputs = []      \n",
    "        def hook(module, input, output):\n",
    "            outputs.append(output)    \n",
    "        for name, module in self.named_children():\n",
    "            if name == layer:\n",
    "                handle = module.register_forward_hook(hook)     \n",
    "        out = self(data)\n",
    "        if not neuron is None:\n",
    "            outputs[0] = outputs[0][0][neuron]\n",
    "        else:\n",
    "            outputs[0] = outputs[0][0]\n",
    "        handle.remove()\n",
    "        return torch.stack(outputs)\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(4 * 4 * 50, 500)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4 * 4 * 50)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "    def extract_outputs(self, data, layer, neuron=None):\n",
    "        outputs = []      \n",
    "        def hook(module, input, output):\n",
    "            outputs.append(output)    \n",
    "        for name, module in self.named_children():\n",
    "            if name == layer:\n",
    "                handle = module.register_forward_hook(hook)     \n",
    "        out = self(data)\n",
    "        if not neuron is None:\n",
    "            outputs[0] = outputs[0][0][neuron]\n",
    "        else:\n",
    "            outputs[0] = outputs[0][0]\n",
    "        handle.remove()\n",
    "        return torch.stack(outputs)\n",
    "  \n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        # calculate robust loss\n",
    "        loss = F.cross_entropy(model(data), target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                       100. * batch_idx / len(train_loader), loss.item()))\n",
    "            \n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "YCva0V7uVIZ_",
    "outputId": "13d39459-552f-48fc-da69-422f96527810"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.301512\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.191104\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.806484\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.701382\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.419531\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.603290\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.135957\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.227602\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.336506\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.158178\n",
      "\n",
      "Test set: Average loss: 0.1832, Accuracy: 9451/10000 (95%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.309578\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.161466\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.109582\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.126139\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.201897\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.079430\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.102752\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.057112\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.056694\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.083487\n",
      "\n",
      "Test set: Average loss: 0.1075, Accuracy: 9670/10000 (97%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.111349\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.098657\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.069383\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.120272\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.054546\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.052148\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.074419\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.111600\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.053788\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.039246\n",
      "\n",
      "Test set: Average loss: 0.0850, Accuracy: 9709/10000 (97%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.056494\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.074563\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.096451\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.061098\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.017499\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.025109\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.017309\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.096200\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.043357\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.087850\n",
      "\n",
      "Test set: Average loss: 0.0623, Accuracy: 9799/10000 (98%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.045850\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.065989\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.097246\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.058882\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.056996\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.087589\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.115934\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.229605\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.099274\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.041458\n",
      "\n",
      "Test set: Average loss: 0.0563, Accuracy: 9823/10000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = ConvNet().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "# check to see if we can just load a previous model\n",
    "%mkdir models\n",
    "latest_model = None\n",
    "m_type = model.__class__.__name__\n",
    "prev_models = glob.glob('/content/models/*'+ m_type +'*.pth')\n",
    "if prev_models:\n",
    "    latest_model = max(prev_models, key=os.path.getctime)\n",
    "\n",
    "if latest_model is not None and m_type in latest_model:\n",
    "    print('loading model', latest_model)\n",
    "    model.load_state_dict(torch.load(latest_model))  \n",
    "else:\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        train(model, device, train_loader, optimizer, epoch)\n",
    "        test(model, device, test_loader)    \n",
    "    torch.save(model.state_dict(), '/content/models/model_' + m_type + '_' +str(datetime.datetime.now()).replace(':','.') + '.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_xXXilwbZq-v"
   },
   "source": [
    "# Attack Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Im4wutR6ge14"
   },
   "outputs": [],
   "source": [
    "def atanh(x, eps=1e-2):\n",
    "    \"\"\"\n",
    "    The inverse hyperbolic tangent function, missing in pytorch.\n",
    "\n",
    "    :param x: a tensor or a Variable\n",
    "    :param eps: used to enhance numeric stability\n",
    "    :return: :math:`\\\\tanh^{-1}{x}`, of the same type as ``x``\n",
    "    \"\"\"\n",
    "    x = x * (1 - eps)\n",
    "    return 0.5 * torch.log((1.0 + x) / (1.0 - x))\n",
    "\n",
    "def to_tanh_space(x, box=box):\n",
    "    \"\"\"\n",
    "    Convert a batch of tensors to tanh-space. This method complements the\n",
    "    implementation of the change-of-variable trick in terms of tanh.\n",
    "\n",
    "    :param x: the batch of tensors, of dimension [B x C x H x W]\n",
    "    :param box: a tuple of lower bound and upper bound of the box constraint\n",
    "    :return: the batch of tensors in tanh-space, of the same dimension;\n",
    "             the returned tensor is on the same device as ``x``\n",
    "    \"\"\"\n",
    "    _box_mul = (box[1] - box[0]) * 0.5\n",
    "    _box_plus = (box[1] + box[0]) * 0.5\n",
    "    return atanh((x - _box_plus) / _box_mul)\n",
    "\n",
    "def from_tanh_space(x, box=box):\n",
    "    \"\"\"\n",
    "    Convert a batch of tensors from tanh-space to oridinary image space.\n",
    "    This method complements the implementation of the change-of-variable trick\n",
    "    in terms of tanh.\n",
    "\n",
    "    :param x: the batch of tensors, of dimension [B x C x H x W]\n",
    "    :param box: a tuple of lower bound and upper bound of the box constraint\n",
    "    :return: the batch of tensors in ordinary image space, of the same\n",
    "             dimension; the returned tensor is on the same device as ``x``\n",
    "    \"\"\"\n",
    "    _box_mul = (box[1] - box[0]) * 0.5\n",
    "    _box_plus = (box[1] + box[0]) * 0.5\n",
    "    return torch.tanh(x) * _box_mul + _box_plus\n",
    "  \n",
    "def compensate_confidence(outputs, targets):\n",
    "    \"\"\"\n",
    "    Compensate for ``self.confidence`` and returns a new weighted sum\n",
    "    vector.\n",
    "\n",
    "    :param outputs: the weighted sum right before the last layer softmax\n",
    "           normalization, of dimension [B x M]\n",
    "    :type outputs: np.ndarray\n",
    "    :param targets: either the attack targets or the real image labels,\n",
    "           depending on whether or not ``self.targeted``, of dimension [B]\n",
    "    :type targets: np.ndarray\n",
    "    :return: the compensated weighted sum of dimension [B x M]\n",
    "    :rtype: np.ndarray\n",
    "    \"\"\"\n",
    "    outputs_comp = outputs.clone()\n",
    "    rng = torch.range(start=0, end=targets.shape[0]-1, dtype=torch.long, device=device)\n",
    "    # targets = targets.int()\n",
    "    if targeted:\n",
    "        # for each image $i$:\n",
    "        # if targeted, `outputs[i, target_onehot]` should be larger than\n",
    "        # `max(outputs[i, ~target_onehot])` by `self.confidence`\n",
    "        outputs_comp[rng, targets] -= confidence\n",
    "    else:\n",
    "        # for each image $i$:\n",
    "        # if not targeted, `max(outputs[i, ~target_onehot]` should be larger\n",
    "        # than `outputs[i, target_onehot]` (the ground truth image labels)\n",
    "        # by `self.confidence`\n",
    "        outputs_comp[rng, targets] += confidence\n",
    "    return outputs_comp\n",
    "  \n",
    "def attack_successful(prediction, target):\n",
    "    \"\"\"\n",
    "    See whether the underlying attack is successful.\n",
    "\n",
    "    :param prediction: the prediction of the model on an input\n",
    "    :type prediction: int\n",
    "    :param target: either the attack target or the ground-truth image label\n",
    "    :type target: int\n",
    "    :return: ``True`` if the attack is successful\n",
    "    :rtype: bool\n",
    "    \"\"\"\n",
    "    if targeted:\n",
    "        return prediction == target\n",
    "    else:\n",
    "        return prediction != target\n",
    "      \n",
    "def norm_divergence(data, model, layer, neuron=None, regularizer_weight=None):\n",
    "    \"\"\"\n",
    "    returns the kld between the activations of the specified layer and a uniform pdf\n",
    "    \"\"\"\n",
    "    # extract layer activations as numpy array\n",
    "    layer_activations = model.extract_outputs(data=data, layer=layer)\n",
    "    \n",
    "    # normalize with softmax (to get a probability density)\n",
    "    out_norm = torch.sum(layer_activations, 0)\n",
    "    out_norm = F.softmax(out_norm, 0)\n",
    "\n",
    "    # create uniform tensor\n",
    "    uniform_tensor = torch.FloatTensor(*out_norm.shape).uniform_(0., 1.).to(device)\n",
    "\n",
    "    # normalize over summation (to get a probability density)\n",
    "    uni_norm = uniform_tensor/torch.sum(uniform_tensor)\n",
    "    \n",
    "    # measure divergence between normalized layer activations and uniform distribution\n",
    "    divergence = F.kl_div(input=out_norm.log(), target=uni_norm, reduction='sum')\n",
    "    \n",
    "    # default regularizer if not provided\n",
    "    if regularizer_weight is None:\n",
    "        regularizer_weight = 0.005 \n",
    "    \n",
    "    return regularizer_weight * divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X7o_T3B3dWzu"
   },
   "outputs": [],
   "source": [
    "# targets = true labels only for when you're doing a targeted attack\n",
    "# otherwise, you're going to make the inputs easier to classify to \n",
    "# do a targeted attack, targets should be some class other than\n",
    "# the true label\n",
    "\n",
    "inputs, targets = next(iter(test_loader))\n",
    "\n",
    "inputs = inputs.to(device)\n",
    "targets = targets.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KiGJy0r2h4aJ"
   },
   "outputs": [],
   "source": [
    "def cw_l2_attack(model, inputs, targets, targeted=False, confidence=0.0,\n",
    "                 c_range=(1e-3, 1e10), search_steps=5, max_steps=1000, \n",
    "                 abort_early=True, box=(-1., 1.), optimizer_lr=1e-2, \n",
    "                 init_rand=False, log_frequency=10):\n",
    "\n",
    "    batch_size = inputs.size(0)\n",
    "    num_classes = model(torch.tensor(inputs[0][None,:], requires_grad=False)).size(1)\n",
    "\n",
    "    # `lower_bounds`, `upper_bounds` and `scale_consts` are used\n",
    "    # for binary search of each `scale_const` in the batch. The element-wise\n",
    "    # inquality holds: lower_bounds < scale_consts <= upper_bounds\n",
    "    lower_bounds = torch.tensor(np.zeros(batch_size), dtype=torch.float, device=device)\n",
    "    upper_bounds = torch.tensor(np.ones(batch_size) * c_range[1], dtype=torch.float, device=device)\n",
    "    scale_consts = torch.tensor(np.ones(batch_size) * c_range[0], dtype=torch.float, device=device)\n",
    "\n",
    "    # Optimal attack to be found.\n",
    "    # The three \"placeholders\" are defined as:\n",
    "    # - `o_best_l2`          : the least L2 norms\n",
    "    # - `o_best_l2_ppred`    : the perturbed predictions made by the adversarial perturbations with the least L2 norms\n",
    "    # - `o_best_adversaries` : the underlying adversarial example of `o_best_l2_ppred`\n",
    "    o_best_l2 = torch.tensor(np.ones(batch_size) * np.inf, dtype=torch.float, device=device)\n",
    "    o_best_l2_ppred = torch.tensor(-np.ones(batch_size), dtype=torch.float, device=device)\n",
    "    o_best_adversaries = inputs.clone()\n",
    "\n",
    "    # convert `inputs` to tanh-space\n",
    "    inputs_tanh = to_tanh_space(inputs)\n",
    "    targets_oh = F.one_hot(targets).float()\n",
    "\n",
    "    # the perturbation tensor (only one we need to track gradients on)\n",
    "    pert_tanh = torch.zeros(inputs.size(), device=device, requires_grad=True)\n",
    "\n",
    "    optimizer = optim.Adam([pert_tanh], lr=optimizer_lr)\n",
    "\n",
    "    for const_step in range(search_steps):\n",
    "\n",
    "        print('Step', const_step)\n",
    "\n",
    "        # the minimum L2 norms of perturbations found during optimization\n",
    "        best_l2 = torch.tensor(np.ones(batch_size) * np.inf, dtype=torch.float, device=device)\n",
    "\n",
    "        # the perturbed predictions made by the adversarial perturbations with the least L2 norms\n",
    "        best_l2_ppred = torch.tensor(-np.ones(batch_size), dtype=torch.float, device=device)\n",
    "\n",
    "        # previous (summed) batch loss, to be used in early stopping policy\n",
    "        prev_batch_loss = torch.tensor(np.inf, device=device)\n",
    "        ae_tol = torch.tensor(1e-4, device=device)\n",
    "\n",
    "        # optimization steps\n",
    "        for optim_step in range(max_steps):\n",
    "\n",
    "            adversaries = from_tanh_space(inputs_tanh + pert_tanh)\n",
    "            pert_outputs = model(adversaries)\n",
    "\n",
    "            # Calculate L2 norm between adversaries and original inputs\n",
    "            pert_norms = torch.pow(adversaries - inputs, exponent=2)\n",
    "            pert_norms = torch.sum(pert_norms.view(pert_norms.size(0), -1), 1)\n",
    "\n",
    "            target_activ = torch.sum(targets_oh * pert_outputs, 1)\n",
    "            maxother_activ = torch.max(((1 - targets_oh) * pert_outputs - targets_oh * 1e4), 1)[0]\n",
    "\n",
    "            if targeted:           \n",
    "                # if targeted, optimize to make `target_activ` larger than `maxother_activ` by `confidence`\n",
    "                f = torch.clamp(maxother_activ - target_activ + confidence, min=0.0)\n",
    "            else:\n",
    "                # if not targeted, optimize to make `maxother_activ` larger than `target_activ` (the ground truth image labels) by `confidence`\n",
    "                f = torch.clamp(target_activ - maxother_activ + confidence, min=0.0)\n",
    "\n",
    "            # the total loss of current batch, should be of dimension [1]\n",
    "            batch_loss = torch.sum(pert_norms + scale_consts * f)\n",
    "\n",
    "            # Do optimization for one step\n",
    "            optimizer.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # \"returns\" batch_loss, pert_norms, pert_outputs, adversaries\n",
    "\n",
    "            if optim_step % log_frequency == 0: \n",
    "                print('batch [{}] loss: {}'.format(optim_step, batch_loss))\n",
    "\n",
    "            if abort_early and not optim_step % (max_steps // 10):   \n",
    "                if batch_loss > prev_batch_loss * (1 - ae_tol):\n",
    "                    break\n",
    "                prev_batch_loss = batch_loss\n",
    "\n",
    "            # update best attack found during optimization\n",
    "            pert_predictions = torch.argmax(pert_outputs, dim=1)\n",
    "            comp_pert_predictions = torch.argmax(compensate_confidence(pert_outputs, targets), dim=1)\n",
    "            for i in range(batch_size):\n",
    "                l2 = pert_norms[i]\n",
    "                cppred = comp_pert_predictions[i]\n",
    "                ppred = pert_predictions[i]\n",
    "                tlabel = targets[i]\n",
    "                ax = adversaries[i]\n",
    "                if attack_successful(cppred, tlabel):\n",
    "                    assert cppred == ppred\n",
    "                    if l2 < best_l2[i]:\n",
    "                        best_l2[i] = l2\n",
    "                        best_l2_ppred[i] = ppred\n",
    "                    if l2 < o_best_l2[i]:\n",
    "                        o_best_l2[i] = l2\n",
    "                        o_best_l2_ppred[i] = ppred\n",
    "                        o_best_adversaries[i] = ax\n",
    "\n",
    "        # binary search of `scale_const`\n",
    "        for i in range(batch_size):\n",
    "            tlabel = targets[i]\n",
    "            if best_l2_ppred[i] != -1:\n",
    "                # successful: attempt to lower `scale_const` by halving it\n",
    "                if scale_consts[i] < upper_bounds[i]:\n",
    "                    upper_bounds[i] = scale_consts[i]\n",
    "                # `upper_bounds[i] == c_range[1]` implies no solution\n",
    "                # found, i.e. upper_bounds[i] has never been updated by\n",
    "                # scale_consts[i] until `scale_consts[i] > 0.1 * c_range[1]`\n",
    "                if upper_bounds[i] < c_range[1] * 0.1:\n",
    "                    scale_consts[i] = (lower_bounds[i] + upper_bounds[i]) / 2\n",
    "            else:\n",
    "                # failure: multiply `scale_const` by ten if no solution\n",
    "                # found; otherwise do binary search\n",
    "                if scale_consts[i] > lower_bounds[i]:\n",
    "                    lower_bounds[i] = scale_consts[i]\n",
    "                if upper_bounds[i] < c_range[1] * 0.1:\n",
    "                    scale_consts[i] = (lower_bounds[i] + upper_bounds[i]) / 2\n",
    "                else:\n",
    "                    scale_consts[i] *= 10\n",
    "                    \n",
    "    return o_best_adversaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VbAGYjiGODjF"
   },
   "outputs": [],
   "source": [
    "targeted=False\n",
    "confidence=0.0\n",
    "c_range=(1e-3, 1e10)\n",
    "search_steps=10\n",
    "max_steps=1000\n",
    "abort_early=True\n",
    "optimizer_lr=5e-4\n",
    "\n",
    "mean = (0.1307,) # the mean used in inputs normalization\n",
    "std = (0.3081,) # the standard deviation used in inputs normalization\n",
    "box = (min((0 - m) / s for m, s in zip(mean, std)),\n",
    "       max((1 - m) / s for m, s in zip(mean, std)))\n",
    "\n",
    "log_frequency = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 433
    },
    "colab_type": "code",
    "id": "kLueeAz4_-MO",
    "outputId": "8a3cedb3-4761-4850-d961-3a5a1010c652"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0\n",
      "batch [0] loss: 10.415900230407715\n",
      "batch [100] loss: 0.7689923644065857\n",
      "batch [200] loss: 0.768934965133667\n",
      "Step 1\n",
      "batch [0] loss: 7.684222221374512\n",
      "batch [100] loss: 7.638064384460449\n",
      "batch [200] loss: 7.638099193572998\n",
      "Step 2\n",
      "batch [0] loss: 75.86489868164062\n",
      "batch [100] loss: 71.02244567871094\n",
      "batch [200] loss: 71.02180480957031\n",
      "Step 3\n",
      "batch [0] loss: 653.8789672851562\n",
      "batch [100] loss: 258.825927734375\n",
      "batch [200] loss: 256.5467529296875\n",
      "batch [300] loss: 255.69586181640625\n",
      "batch [400] loss: 255.42002868652344\n",
      "batch [500] loss: 254.77783203125\n",
      "batch [600] loss: 255.2265625\n",
      "Step 4\n",
      "batch [0] loss: 359.650146484375\n",
      "batch [100] loss: 242.97149658203125\n",
      "batch [200] loss: 244.4771728515625\n"
     ]
    }
   ],
   "source": [
    "cw_advs = cw_l2_attack(model, inputs, targets, targeted=False, confidence=0.0,\n",
    "                       c_range=(1e-3, 1e10), search_steps=5, max_steps=1000, \n",
    "                       abort_early=True, box=box, optimizer_lr=1e-2, \n",
    "                       init_rand=False, log_frequency=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 315
    },
    "colab_type": "code",
    "id": "CUsvsyleV1Is",
    "outputId": "8bdaf0ca-903b-4871-c249-456f949ae275"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perturbed Accuracy: 0/100 (0%)\n",
      "\n",
      "Original Accuracy: 98/100 (98%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAADkCAYAAADNX7BjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pert_output = model(cw_advs)\n",
    "orig_output = model(inputs)\n",
    "\n",
    "pert_pred = torch.argmax(pert_output, dim=1)\n",
    "orig_pred = torch.argmax(orig_output, dim=1)\n",
    "\n",
    "pert_correct = pert_pred.eq(targets.data).sum()\n",
    "orig_correct = orig_pred.eq(targets.data).sum()\n",
    "\n",
    "pert_acc = 100. * pert_correct / len(targets)\n",
    "orig_acc = 100. * orig_correct / len(targets)\n",
    "\n",
    "print('Perturbed Accuracy: {}/{} ({:.0f}%)\\n'.format(pert_correct, len(targets), pert_acc))\n",
    "print('Original Accuracy: {}/{} ({:.0f}%)\\n'.format(orig_correct, len(targets), orig_acc))\n",
    "\n",
    "adversarial_examples = o_best_adversaries.cpu().detach().numpy()\n",
    "input_examples = inputs.cpu().detach().numpy()\n",
    "\n",
    "# inputs, adversarial_examples, targets\n",
    "num_samples = 5\n",
    "\n",
    "for i in range(1,num_samples+1):\n",
    "    \n",
    "    plt.subplot(2, num_samples, i)\n",
    "    plt.imshow(np.squeeze(input_examples[i]), cmap='gray')  \n",
    "    plt.title('actual {}'.format(targets[i].item()))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    \n",
    "    plt.subplot(2, num_samples, num_samples+i)\n",
    "    plt.imshow(np.squeeze(adversarial_examples[i]), cmap='gray')\n",
    "    plt.title('{} {}'.format(pert_pred[i].item(), orig_pred[i].item()))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "colab_type": "code",
    "id": "TLhycjrBPl3j",
    "outputId": "080b9c4b-e448-4118-b69b-e7f3ab829ca9"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEICAYAAACQ6CLfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ng7fzKbjcAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow((o_best_adversaries[1] - inputs[1]).cpu().detach().numpy().reshape(28,28), cmap='gray') \n",
    "plt.title(targets[1].cpu().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4PMv-KEuX9Q2"
   },
   "source": [
    "# Diversity Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MiwScd1eX1eJ"
   },
   "outputs": [],
   "source": [
    "def cw_div_attack(model, inputs, targets, targeted=False, confidence=0.0,\n",
    "                  c_range=(1e-3, 1e10), search_steps=5, max_steps=1000, \n",
    "                  abort_early=True, box=(-1., 1.), optimizer_lr=1e-2, \n",
    "                  init_rand=False, log_frequency=10):\n",
    "\n",
    "    batch_size = inputs.size(0)\n",
    "    num_classes = model(torch.tensor(inputs[0][None,:], requires_grad=False)).size(1)\n",
    "\n",
    "    # `lower_bounds`, `upper_bounds` and `scale_consts` are used\n",
    "    # for binary search of each `scale_const` in the batch. The element-wise\n",
    "    # inquality holds: lower_bounds < scale_consts <= upper_bounds\n",
    "    lower_bounds = torch.tensor(np.zeros(batch_size), dtype=torch.float, device=device)\n",
    "    upper_bounds = torch.tensor(np.ones(batch_size) * c_range[1], dtype=torch.float, device=device)\n",
    "    scale_consts = torch.tensor(np.ones(batch_size) * c_range[0], dtype=torch.float, device=device)\n",
    "\n",
    "    # Optimal attack to be found.\n",
    "    # The three \"placeholders\" are defined as:\n",
    "    # - `o_best_div`         : the least divergences\n",
    "    # - `o_best_div_ppred`   : the perturbed predictions made by the adversarial perturbations with the least divergences\n",
    "    # - `o_best_adversaries` : the underlying adversarial example of `o_best_div_ppred`\n",
    "    o_best_div = torch.tensor(np.ones(batch_size) * np.inf, dtype=torch.float, device=device)\n",
    "    o_best_div_ppred = torch.tensor(-np.ones(batch_size), dtype=torch.float, device=device)\n",
    "    o_best_adversaries = inputs.clone()\n",
    "\n",
    "    # convert `inputs` to tanh-space\n",
    "    inputs_tanh = to_tanh_space(inputs)\n",
    "    targets_oh = F.one_hot(targets).float()\n",
    "\n",
    "    # the perturbation tensor (only one we need to track gradients on)\n",
    "    pert_tanh = torch.zeros(inputs.size(), device=device, requires_grad=True)\n",
    "\n",
    "    optimizer = optim.Adam([pert_tanh], lr=optimizer_lr)\n",
    "\n",
    "    for const_step in range(search_steps):\n",
    "\n",
    "        print('Step', const_step)\n",
    "\n",
    "        # the minimum divergences of perturbations found during optimization\n",
    "        best_div = torch.tensor(np.ones(batch_size) * np.inf, dtype=torch.float, device=device)\n",
    "\n",
    "        # the perturbed predictions made by the adversarial perturbations with the least divergences\n",
    "        best_div_ppred = torch.tensor(-np.ones(batch_size), dtype=torch.float, device=device)\n",
    "\n",
    "        # previous (summed) batch loss, to be used in early stopping policy\n",
    "        prev_batch_loss = torch.tensor(np.inf, device=device)\n",
    "        ae_tol = torch.tensor(1e-4, device=device)\n",
    "\n",
    "        # optimization steps\n",
    "        for optim_step in range(max_steps):\n",
    "\n",
    "            adversaries = from_tanh_space(inputs_tanh + pert_tanh)\n",
    "            pert_outputs = model(adversaries)\n",
    "\n",
    "            # calculate kl divergence for each input\n",
    "            divs = []\n",
    "            for i in range(batch_size):\n",
    "                divs.append(norm_divergence(data=adversaries[i].unsqueeze(0), model=model, layer='relu3', regularizer_weight=1))\n",
    "\n",
    "            div_norms = torch.tensor(torch.stack(divs), device=device)\n",
    "\n",
    "            target_activ = torch.sum(targets_oh * pert_outputs, 1)\n",
    "            maxother_activ = torch.max(((1 - targets_oh) * pert_outputs - targets_oh * 1e4), 1)[0]\n",
    "\n",
    "            if targeted:           \n",
    "                # if targeted, optimize to make `target_activ` larger than `maxother_activ` by `confidence`\n",
    "                f = torch.clamp(maxother_activ - target_activ + confidence, min=0.0)\n",
    "            else:\n",
    "                # if not targeted, optimize to make `maxother_activ` larger than `target_activ` (the ground truth image labels) by `confidence`\n",
    "                f = torch.clamp(target_activ - maxother_activ + confidence, min=0.0)\n",
    "\n",
    "    #         # diversity regularizer\n",
    "    #         diversity_reg = norm_divergence(data=adversaries, model=model, layer='relu3', regularizer_weight=1)\n",
    "\n",
    "            # the total loss of current batch, should be of dimension [1]\n",
    "            batch_loss = torch.sum(scale_consts * f + div_norms) # + diversity_reg\n",
    "\n",
    "            # Do optimization for one step\n",
    "            optimizer.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # \"returns\" batch_loss, pert_norms, pert_outputs, adversaries\n",
    "\n",
    "            if optim_step % log_frequency == 0: \n",
    "                print('batch [{}] loss: {}'.format(optim_step, batch_loss))\n",
    "\n",
    "            if abort_early and not optim_step % (max_steps // 10):   \n",
    "                if batch_loss > prev_batch_loss * (1 - ae_tol):\n",
    "                    break\n",
    "                prev_batch_loss = batch_loss\n",
    "\n",
    "            # update best attack found during optimization\n",
    "            pert_predictions = torch.argmax(pert_outputs, dim=1)\n",
    "            comp_pert_predictions = torch.argmax(compensate_confidence(pert_outputs, targets), dim=1)\n",
    "            for i in range(batch_size):\n",
    "                div = div_norms[i]\n",
    "                cppred = comp_pert_predictions[i]\n",
    "                ppred = pert_predictions[i]\n",
    "                tlabel = targets[i]\n",
    "                ax = adversaries[i]\n",
    "                if attack_successful(cppred, tlabel):\n",
    "                    assert cppred == ppred\n",
    "                    if div < best_div[i]:\n",
    "                        best_div[i] = div\n",
    "                        best_div_ppred[i] = ppred\n",
    "                    if div < o_best_div[i]:\n",
    "                        o_best_div[i] = div\n",
    "                        o_best_div_ppred[i] = ppred\n",
    "                        o_best_adversaries[i] = ax\n",
    "\n",
    "        # binary search of `scale_const`\n",
    "        for i in range(batch_size):\n",
    "            tlabel = targets[i]\n",
    "            if best_div_ppred[i] != -1:\n",
    "                # successful: attempt to lower `scale_const` by halving it\n",
    "                if scale_consts[i] < upper_bounds[i]:\n",
    "                    upper_bounds[i] = scale_consts[i]\n",
    "                # `upper_bounds[i] == c_range[1]` implies no solution\n",
    "                # found, i.e. upper_bounds[i] has never been updated by\n",
    "                # scale_consts[i] until `scale_consts[i] > 0.1 * c_range[1]`\n",
    "                if upper_bounds[i] < c_range[1] * 0.1:\n",
    "                    scale_consts[i] = (lower_bounds[i] + upper_bounds[i]) / 2\n",
    "            else:\n",
    "                # failure: multiply `scale_const` by ten if no solution\n",
    "                # found; otherwise do binary search\n",
    "                if scale_consts[i] > lower_bounds[i]:\n",
    "                    lower_bounds[i] = scale_consts[i]\n",
    "                if upper_bounds[i] < c_range[1] * 0.1:\n",
    "                    scale_consts[i] = (lower_bounds[i] + upper_bounds[i]) / 2\n",
    "                else:\n",
    "                    scale_consts[i] *= 10\n",
    "                    \n",
    "    return o_best_adversaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 643
    },
    "colab_type": "code",
    "id": "BU2t6ERVB6C3",
    "outputId": "0fd417a3-2d09-4e8d-a623-90ff03b7d3bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0\n",
      "batch [0] loss: 135.1306915283203\n",
      "batch [100] loss: 125.01528930664062\n",
      "batch [200] loss: 124.4491958618164\n",
      "batch [300] loss: 124.50749969482422\n",
      "Step 1\n",
      "batch [0] loss: 124.57865142822266\n",
      "batch [100] loss: 124.82147216796875\n",
      "Step 2\n",
      "batch [0] loss: 125.05036926269531\n",
      "batch [100] loss: 124.69972229003906\n",
      "batch [200] loss: 124.65570831298828\n",
      "batch [300] loss: 124.74939727783203\n",
      "Step 3\n",
      "batch [0] loss: 124.8875732421875\n",
      "batch [100] loss: 124.60482788085938\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-4fcbb7cc2e76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m                             \u001b[0mc_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearch_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                             \u001b[0mabort_early\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                             init_rand=False, log_frequency=100)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-89e3339add20>\u001b[0m in \u001b[0;36mcw_div_attack\u001b[0;34m(model, inputs, targets, targeted, confidence, c_range, search_steps, max_steps, abort_early, box, optimizer_lr, init_rand, log_frequency)\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;31m# Do optimization for one step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0mbatch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 14.73 GiB total capacity; 13.93 GiB already allocated; 13.94 MiB free; 27.72 MiB cached)"
     ]
    }
   ],
   "source": [
    "cw_advs_div = cw_div_attack(model, inputs, targets, targeted=False, confidence=0.0,\n",
    "                            c_range=(1e-3, 1e10), search_steps=5, max_steps=1000, \n",
    "                            abort_early=True, box=box, optimizer_lr=1e-2, \n",
    "                            init_rand=False, log_frequency=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c651LtJEbi56"
   },
   "outputs": [],
   "source": [
    "pert_output = model(cw_advs_div)\n",
    "orig_output = model(inputs)\n",
    "\n",
    "pert_pred = torch.argmax(pert_output, dim=1)\n",
    "orig_pred = torch.argmax(orig_output, dim=1)\n",
    "\n",
    "pert_correct = pert_pred.eq(targets.data).sum()\n",
    "orig_correct = orig_pred.eq(targets.data).sum()\n",
    "\n",
    "pert_acc = 100. * pert_correct / len(targets)\n",
    "orig_acc = 100. * orig_correct / len(targets)\n",
    "\n",
    "print('Perturbed Accuracy: {}/{} ({:.0f}%)\\n'.format(pert_correct, len(targets), pert_acc))\n",
    "print('Original Accuracy: {}/{} ({:.0f}%)\\n'.format(orig_correct, len(targets), orig_acc))\n",
    "\n",
    "adversarial_examples = o_best_adversaries.cpu().detach().numpy()\n",
    "input_examples = inputs.cpu().detach().numpy()\n",
    "\n",
    "# inputs, adversarial_examples, targets\n",
    "num_samples = 5\n",
    "\n",
    "for i in range(1,num_samples+1):\n",
    "    \n",
    "    plt.subplot(2, num_samples, i)\n",
    "    plt.imshow(np.squeeze(input_examples[i]), cmap='gray')  \n",
    "    plt.title('actual {}'.format(targets[i].item()))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    \n",
    "    plt.subplot(2, num_samples, num_samples+i)\n",
    "    plt.imshow(np.squeeze(adversarial_examples[i]), cmap='gray')\n",
    "    plt.title('{} {}'.format(pert_pred[i].item(), orig_pred[i].item()))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RcyIPslXb0IJ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CW - pytorch.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
